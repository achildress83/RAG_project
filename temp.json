{"type": "UncategorizedText", "element_id": "f50ad2eb90733ac345a808a1a9e6e75f", "text": "4 2 0 2", "metadata": {"filetype": "application/pdf", "languages": ["eng"], "page_number": 1, "filename": "rtx_paper.pdf"}}
{"type": "NarrativeText", "element_id": "2f63fe6c77d28191ae1d69daea290234", "text": "r p A 2", "metadata": {"filetype": "application/pdf", "languages": ["eng"], "page_number": 1, "filename": "rtx_paper.pdf"}}
{"type": "UncategorizedText", "element_id": "7630b82f56224e713057fe23dbfc1122", "text": "]", "metadata": {"filetype": "application/pdf", "languages": ["eng"], "page_number": 1, "filename": "rtx_paper.pdf"}}
{"type": "Title", "element_id": "735c6af3e3d8a967022eda94d9e1434f", "text": "O R . s c [", "metadata": {"filetype": "application/pdf", "languages": ["eng"], "page_number": 1, "filename": "rtx_paper.pdf"}}
{"type": "UncategorizedText", "element_id": "661298c7e2144c15579c599385497ab9", "text": "6 v 4 6 8 8 0 . 0 1 3 2 : v i X r a", "metadata": {"filetype": "application/pdf", "languages": ["eng"], "page_number": 1, "parent_id": "735c6af3e3d8a967022eda94d9e1434f", "filename": "rtx_paper.pdf"}}
{"type": "UncategorizedText", "element_id": "bd57b596576ece5d43181c3f0e568935", "text": "Open X-Embodiment: Robotic Learning Datasets and RT-X Models Open X-Embodiment Collaboration0 robotics-transformer-x.github.io Abby O\u2019Neill32, Abdul Rehman35, Abhiram Maddukuri43, Abhishek Gupta44, Abhishek Padalkar10, Abraham Lee32, Acorn Pooley11, Agrim Gupta27, Ajay Mandlekar21, Ajinkya Jain15, Albert Tung27, Alex Bewley11, Alex Herzog11, Alex Irpan11, Alexander Khazatsky27, Anant Rai22, Anchit Gupta19, Andrew Wang32, Anikait Singh11,32, Animesh Garg9, Aniruddha Kembhavi1, Annie Xie27, Anthony Brohan11, Antonin Raffin10, Archit Sharma27, Arefeh Yavary33, Arhan Jain44, Ashwin Balakrishna31, Ayzaan Wahid11, Ben Burgess-Limerick24, Beomjoon Kim17, Bernhard Sch\u00a8olkopf18, Blake Wulfe31, Brian Ichter11, Cewu Lu26,8, Charles Xu32, Charlotte Le32, Chelsea Finn11,27, Chen Wang27, Chenfeng Xu32, Cheng Chi5,27, Chenguang Huang36, Christine Chan11, Christopher Agia27, Chuer Pan27, Chuyuan Fu11, Coline Devin11, Danfei Xu9, Daniel Morton27, Danny Driess11, Daphne Chen44, Deepak Pathak4, Dhruv Shah32, Dieter B\u00a8uchler18, Dinesh Jayaraman40, Dmitry Kalashnikov11, Dorsa Sadigh11, Edward Johns14, Ethan Foster27, Fangchen Liu32, Federico Ceola16, Fei Xia11, Feiyu Zhao13, Freek Stulp10, Gaoyue Zhou22, Gaurav S. Sukhatme41, Gautam Salhotra41,15, Ge Yan34, Gilbert Feng32, Giulio Schiavi7, Glen Berseth39,20, Gregory Kahn32, Guanzhi Wang3,21, Hao Su34, Hao-Shu Fang26, Haochen Shi27, Henghui Bao41, Heni Ben Amor2, Henrik I Christensen34, Hiroki Furuta30, Homer Walke32, Hongjie Fang26, Huy Ha5,27, Igor Mordatch11, Ilija Radosavovic32, Isabel Leal11, Jacky Liang11, Jad Abou-Chakra24, Jaehyung Kim17, Jaimyn Drake32, Jan Peters28, Jan Schneider18, Jasmine Hsu11, Jeannette Bohg27, Jeffrey Bingham11, Jeffrey Wu32, Jensen Gao27, Jiaheng Hu29, Jiajun Wu27, Jialin Wu12, Jiankai Sun27, Jianlan Luo32, Jiayuan Gu34, Jie Tan11, Jihoon Oh30, Jimmy Wu23, Jingpei Lu34, Jingyun Yang27, Jitendra Malik32, Jo\u02dcao Silv\u00b4erio10, Joey Hejna27, Jonathan Booher27, Jonathan Tompson11, Jonathan Yang27, Jordi Salvador1, Joseph J. Lim17, Junhyek Han17, Kaiyuan Wang34, Kanishka Rao11, Karl Pertsch32,27, Karol Hausman11, Keegan Go15, Keerthana Gopalakrishnan11, Ken Goldberg32, Kendra Byrne11, Kenneth Oslund11, Kento Kawaharazuka30, Kevin Black32, Kevin Lin27, Kevin Zhang4, Kiana Ehsani1, Kiran Lekkala41, Kirsty Ellis39, Krishan Rana24, Krishnan Srinivasan27, Kuan Fang32, Kunal Pratap Singh6, Kuo-Hao Zeng1, Kyle Hatch31, Kyle Hsu27, Laurent Itti41, Lawrence Yunliang Chen32, Lerrel Pinto22, Li Fei-Fei27, Liam Tan32, Linxi \u201dJim\u201d Fan21, Lionel Ott7, Lisa Lee11, Luca Weihs1, Magnum Chen13, Marion Lepert27, Marius Memmel44, Masayoshi Tomizuka32, Masha Itkina31, Mateo Guaman Castro44, Max Spero27, Maximilian Du27, Michael Ahn11, Michael C. Yip34, Mingtong Zhang37, Mingyu Ding32, Minho Heo17, Mohan Kumar Srirama4, Mohit Sharma4, Moo Jin Kim27, Naoaki Kanazawa30, Nicklas Hansen34, Nicolas Heess11, Nikhil J Joshi11, Niko Suenderhauf24, Ning Liu13, Norman Di Palo14, Nur Muhammad Mahi Shafiullah22, Oier Mees36, Oliver Kroemer4, Osbert Bastani40, Pannag R Sanketi11, Patrick \u201dTree\u201d Miller31, Patrick Yin44, Paul Wohlhart11, Peng Xu11, Peter David Fagan35, Peter Mitrano38, Pierre Sermanet11, Pieter Abbeel32, Priya Sundaresan27, Qiuyu Chen44, Quan Vuong11, Rafael Rafailov11,27, Ran Tian32, Ria Doshi32, Roberto Mart\u00b4\u0131n-Mart\u00b4\u0131n29, Rohan Baijal44, Rosario Scalise44, Rose Hendrix1, Roy Lin32, Runjia Qian13, Ruohan Zhang27, Russell Mendonca4, Rutav Shah29, Ryan Hoque32, Ryan Julian11, Samuel Bustamante10, Sean Kirmani11, Sergey Levine11,32, Shan Lin34, Sherry Moore11, Shikhar Bahl4, Shivin Dass41,29, Shubham Sonawani2, Shuran Song5, Sichun Xu11, Siddhant Haldar22, Siddharth Karamcheti27, Simeon Adebola32, Simon Guist18, Soroush Nasiriany29, Stefan Schaal15, Stefan Welker11, Stephen Tian27, Subramanian Ramamoorthy35, Sudeep Dasari4, Suneel Belkhale27, Sungjae Park17, Suraj Nair31, Suvir Mirchandani27, Takayuki Osa30, Tanmay Gupta1, Tatsuya Harada30,25, Tatsuya Matsushima30, Ted Xiao11, Thomas Kollar31, Tianhe Yu11, Tianli Ding11, Todor Davchev11, Tony Z. Zhao27, Travis Armstrong11, Trevor Darrell32, Trinity Chung32, Vidhi Jain11,4, Vincent Vanhoucke11, Wei Zhan32, Wenxuan Zhou11,4, Wolfram Burgard42, Xi Chen11, Xiaolong Wang34, Xinghao Zhu32, Xinyang Geng32, Xiyuan Liu13, Xu Liangwei13, Xuanlin Li34, Yao Lu11, Yecheng Jason Ma40, Yejin Kim1, Yevgen Chebotar11, Yifan Zhou2, Yifeng Zhu29, Yilin Wu4, Ying Xu11, Yixuan Wang37, Yonatan Bisk4, Yoonyoung Cho17, Youngwoon Lee32, Yuchen Cui27, Yue Cao13, Yueh-Hua Wu34, Yujin Tang11,30, Yuke Zhu29, Yunchu Zhang44, Yunfan Jiang27, Yunshuang Li40, Yunzhu Li37, Yusuke Iwasawa30, Yutaka Matsuo30, Zehan Ma32, Zhuo Xu11, Zichen Jeff Cui22, Zichen Zhang1, Zipeng Fu27, Zipeng Lin32", "metadata": {"filetype": "application/pdf", "languages": ["eng"], "page_number": 1, "parent_id": "735c6af3e3d8a967022eda94d9e1434f", "filename": "rtx_paper.pdf"}}
{"type": "Image", "element_id": "89eef53e1c83117154f60e8d8bfaaaab", "text": "TOTO from Cable Routing RT-1 QT-Opt 1 @ 1M Episodes 311 Scenes [ ] \u2014 \u2014 L Research Labs across 21 Institutions \u2014 pour pick anything, 22 Embodiments \u2018sweep the green Cloth to the lft \u00a2 side of the table RERE & RS pick green chip bag from counter S ) 527 Skills L A i o \u2014 A A 5 e e A o A= : set the bowl to o1 T8 b L pour stack route the right side of the table sl Stach cups 60 Datasets & place the black block L\u2018. & \ufb01bo wl i the dish rack Y /E. 1) Pog < = A A 1,798 Attributes. 5,228 Objects. 23,486 S elations i = Jaco Play ALOHA Bridge Opening", "metadata": {"filetype": "application/pdf", "languages": ["eng"], "page_number": 1, "filename": "rtx_paper.pdf"}}
{"type": "FigureCaption", "element_id": "18bcb7a37453e85a8adb55d11e0bd7af", "text": "Fig. 1: We propose an open, large-scale dataset for robot learning curated from 21 institutions across the globe. The dataset represents diverse behaviors, robot embodiments and environments, and enables learning generalized robotic policies.", "metadata": {"filetype": "application/pdf", "languages": ["eng"], "page_number": 1, "filename": "rtx_paper.pdf"}}
{"type": "NarrativeText", "element_id": "a0c6406729b498293762aeb147485f7a", "text": "Abstract\u2014 Large, high-capacity models trained on diverse datasets have shown remarkable successes on efficiently tackling downstream applications. In domains from NLP to Computer Vision, this has led to a consolidation of pretrained models, with general pretrained backbones serving as a starting point for many applications. Can such a consolidation happen in robotics? Conventionally, robotic learning methods train a separate model for every application, every robot, and even", "metadata": {"filetype": "application/pdf", "languages": ["eng"], "page_number": 1, "filename": "rtx_paper.pdf"}}
{"type": "NarrativeText", "element_id": "6de355eb52fb5cd3518c83624f2d0e71", "text": "every environment. Can we instead train \u201cgeneralist\u201d X-robot policy that can be adapted efficiently to new robots, tasks, and environments? In this paper, we provide datasets in standardized data formats and models to make it possible to explore this possibility in the context of robotic manipulation, alongside experimental results that provide an example of effective X-robot policies. We assemble a dataset from 22 different robots collected through a collaboration between 21", "metadata": {"filetype": "application/pdf", "languages": ["eng"], "page_number": 1, "filename": "rtx_paper.pdf"}}
{"type": "NarrativeText", "element_id": "5d7e67673be9ee51517d6253cb054e55", "text": "institutions, demonstrating 527 skills (160266 tasks). We show that a high-capacity model trained on this data, which we call RT-X, exhibits positive transfer and improves the capabilities of multiple robots by leveraging experience from other platforms. The project website is robotics-transformer-x.github.io.", "metadata": {"filetype": "application/pdf", "languages": ["eng"], "page_number": 2, "filename": "rtx_paper.pdf"}}
{"type": "NarrativeText", "element_id": "67065e79fcfabcdbeefe2533c86e29c8", "text": "I. INTRODUCTION A central lesson from advances in machine learning and artificial intelligence is that large-scale learning from di- verse datasets can enable capable AI systems by providing for general-purpose pretrained models. In fact, large-scale general-purpose models typically trained on large and di- verse datasets can often outperform their narrowly targeted counterparts trained on smaller but more task-specific data. For instance, open-vocab classifiers (e.g., CLIP [1]) trained on large datasets scraped from the web tend to outperform fixed-vocabulary models trained on more limited datasets, and large language models [2, 3] trained on massive text corpora tend to outperform systems that are only trained on narrow task-specific datasets. Increasingly, the most effective way to tackle a given narrow task (e.g., in vision or NLP) is to adapt a general-purpose model. However, these lessons are difficult to apply in robotics: any single robotic domain might be too narrow, and while computer vision and NLP can leverage large datasets sourced from the web, comparably large and broad datasets for robotic interaction are hard to come by. Even the largest data collection efforts still end up with datasets that are a fraction of the size and diversity of benchmark datasets in vision (5-18M) [4, 5] and NLP (1.5B-4.5B) [6, 7]. More importantly, such datasets are often still narrow along some axes of variation, either focusing on a single environment, a single set of objects, or a narrow range of tasks. How can we overcome these challenges in robotics and move the field of robotic learning toward large data regime that has been so successful in other domains?", "metadata": {"filetype": "application/pdf", "languages": ["eng"], "page_number": 2, "filename": "rtx_paper.pdf"}}
{"type": "NarrativeText", "element_id": "a25690587288fff0ee8a46844f73b4e0", "text": "Inspired by the generalization made possible by pretrain- ing large vision or language models on diverse data, we take the perspective that the goal of training generalizable robot policies requires X-embodiment training, i.e., with data from multiple robotic platforms. While each individual robotic learning dataset might be too narrow, the union of all such datasets provides a better coverage of variations in environments and robots. Learning generalizable robot policies requires developing methods that can utilize X- embodiment data, tapping into datasets from many labs, robots, and settings. Even if such datasets in their current size and coverage are insufficient to attain the impressive", "metadata": {"filetype": "application/pdf", "languages": ["eng"], "page_number": 2, "filename": "rtx_paper.pdf"}}
{"type": "NarrativeText", "element_id": "2f8a8fc6c286ede8ad71157fe4a7621e", "text": "0 1Allen Institute for AI; 2Arizona State University; 3California Institute of Technology; 4Carnegie Mellon University; 5Columbia University; 6EPFL; 7ETH Z\u00a8urich; 8Flexiv Robotics; 9Georgia Institute of Technology; 10German Aerospace Center; 11Google DeepMind; 12Google Research; 13IO-AI TECH; 14Imperial Col- lege London; 15Intrinsic LLC; 16Istituto Italiano di Tecnologia; 17Korea Advanced Institute of Science & Technology; 18Max Planck Institute; 19Meta AI; 20Mila Quebec; 21NVIDIA; 22New York University; 23Princeton University; 24Queensland University of Technology; 25RIKEN; 26Shanghai Jiao Tong University; 27Stanford University; 28Technische Universit\u00a8at Darmstadt; 29The University of Texas at Austin; 30The University of Tokyo; 31Toyota Research Institute; 32University of California, Berkeley; 33University of California, Davis; 34University of California, San Diego; 35University of Edinburgh; 36University of Freiburg; 37University of Illinois Urbana- Champaign; 38University of Michigan; 39University of Montreal; 40University of Pennsylvania; 41University of Southern California; 42University of Technology, Nuremberg; 43University of Texas at Austin; 44University of Washington", "metadata": {"filetype": "application/pdf", "languages": ["eng"], "page_number": 2, "filename": "rtx_paper.pdf"}}
{"type": "NarrativeText", "element_id": "7786c09dc19aa65938cb4537fffbebdd", "text": "generalization results that have been demonstrated by large language models, in the future, the union of such data can potentially provide this kind of coverage. Because of this, we believe that enabling research into X-embodiment robotic learning is critical at the present juncture.", "metadata": {"filetype": "application/pdf", "languages": ["eng"], "page_number": 2, "filename": "rtx_paper.pdf"}}
{"type": "NarrativeText", "element_id": "af2c605feab1658a4551124324242f92", "text": "Following this rationale, we have two goals: (1) Evaluate whether policies trained on data from many different robots and environments enjoy the benefits of positive transfer, attaining better performance than policies trained only on data from each evaluation setup. (2) Organize large robotic datasets to enable future research on X-embodiment models. We focus our work on robotic manipulation. Addressing goal (1), our empirical contribution is to demonstrate that several recent robotic learning methods, with minimal mod- ification, can utilize X-embodiment data and enable positive transfer. Specifically, we train the RT-1 [8] and RT-2 [9] models on 9 different robotic manipulators. We show that the resulting models, which we call RT-X, can improve over policies trained only on data from the evaluation domain, exhibiting better generalization and new capabilities. Ad- dressing (2), we provide the Open X-Embodiment (OXE) Repository, which includes a dataset with 22 different robotic embodiments from 21 different institutions that can enable the robotics community to pursue further research on X- embodiment models, along with open-source tools to facili- tate such research. Our aim is not to innovate in terms of the particular architectures and algorithms, but rather to provide the model that we trained together with data and tools to energize research around X-embodiment robotic learning. II. RELATED WORK Transfer across embodiments. A number of prior works have studied methods for transfer across robot embodiments in simulation [10\u201322] and on real robots [23\u201329]. These methods often introduce mechanisms specifically designed to address the embodiment gap between different robots, such as shared action representations [14, 30], incorporating rep- resentation learning objectives [17, 26], adapting the learned policy on embodiment information [11, 15, 18, 30, 31], and decoupling robot and environment representations [24]. Prior work has provided initial demonstrations of X-embodiment training [27] and transfer [25, 29, 32] with transformer models. We investigate complementary architectures and provide complementary analyses, and, in particular, study the interaction between X-embodiment transfer and web-scale pretraining. Similarly, methods for transfer across human and robot embodiments also often employ techniques for reducing the embodiment gap, i.e. by translating between domains or learning transferable representations [33\u201343]. Al- ternatively, some works focus on sub-aspects of the problem such as learning transferable reward functions [17, 44\u201348], goals [49, 50], dynamics models [51], or visual representa- tions [52\u201359] from human video data. Unlike most of these prior works, we directly train a policy on X-embodiment data, without any mechanisms to reduce the embodiment gap, and observe positive transfer by leveraging that data. Large-scale robot learning datasets. The robot learning community has created open-source robot learning datasets,", "metadata": {"filetype": "application/pdf", "languages": ["eng"], "page_number": 2, "filename": "rtx_paper.pdf"}}
{"type": "Image", "element_id": "7eb04e8cb6b49bb456577b8e9adea2df", "text": "In ;% & FLS \u00a3 Sawyer < & &5 S \u00a7 S 5 $ ;\u201c & \u00a7 e\u2019p & $ K 4 Kinova Gen3 FES & \u00b0 & K4 ey g S \u20ac \u00a2 Hello Stretch B\ufb01cs'l g e i WidowX Jackal WidowX Sawyer \u00a5 (a) # Datasets per Robot Embodiment (b) # Scenes per Embodiment (c) # Trajectories per Embodiment 14000 Shapes el 12000 150000 10000 Containers 125000 5000 100000 ppliances 75000 5000 0000 2000 Utensils 25000 2000 o ] \u00a3 I Lo & S5 59 \u00a2 OF, SIS S oo 3 \u00a7 S \u00a7&8 & & $ & $ ;\ufb01g f@ & & & & SSES & 8 .4 & \u00a3 5 i Food 3;: SEES & \u00a3 \"JY a & @f (d) Common Dataset Skills (e) Common Dataset Objects", "metadata": {"filetype": "application/pdf", "languages": ["eng"], "page_number": 3, "filename": "rtx_paper.pdf"}}
{"type": "FigureCaption", "element_id": "2c143453b1f9280ec4f491f8dfc2ee5b", "text": "Fig. 2: The Open X-Embodiment Dataset. (a): the dataset consists of 60 individual datasets across 22 embodiments. (b): the Franka robot has the largest diversity in visually distinct scenes due to the large number of Franka datasets, (c): xArm and Google Robot contribute the most number of trajectories due to a few large datasets, (d, e): the dataset contains a great diversity of skills and common objects.", "metadata": {"filetype": "application/pdf", "languages": ["eng"], "page_number": 3, "filename": "rtx_paper.pdf"}}
{"type": "NarrativeText", "element_id": "aa6024b9086c252da680944be2c6cf3e", "text": "spanning grasping [60\u201371], pushing interactions [23, 72\u201374], sets of objects and models [75\u201385], and teleoperated demon- strations [8, 86\u201395]. With the exception of RoboNet [23], these datasets contain data of robots of the same type, whereas we focus on data spanning multiple embodiments. The goal of our data repository is complementary to these efforts: we process and aggregate a large number of prior datasets into a single, standardized repository, called Open X-Embodiment, which shows how robot learning datasets can be shared in a meaningul and useful way. Language-conditioned robot learning. Prior work has aimed to endow robots and other agents with the ability to understand and follow language instructions [96\u2013101], often by learning language-conditioned policies [8, 40, 45, 102\u2013 106]. We train language-conditioned policies via imitation learning like many of these prior works but do so using large-scale multi-embodiment demonstration data. Following previous works that leverage pre-trained language embed- dings [8, 40, 45, 103, 107\u2013112] and pre-trained vision- language models [9, 113\u2013115] in robotic imitation learning, we study both forms of pre-training in our experiments, specifically following the recipes of RT-1 [8] and RT-2 [9].", "metadata": {"filetype": "application/pdf", "languages": ["eng"], "page_number": 3, "filename": "rtx_paper.pdf"}}
{"type": "Title", "element_id": "7e8a689d89b024a7dc47ebee59bb07fa", "text": "III. THE OPEN X-EMBODIMENT REPOSITORY", "metadata": {"filetype": "application/pdf", "languages": ["eng"], "page_number": 3, "filename": "rtx_paper.pdf"}}
{"type": "NarrativeText", "element_id": "48a0c5420d4a99fae54d0a7be8b112f2", "text": "the Open X-Embodiment Repository (robotics-transformer-x.github.io) \u2013 an open-source reposi- tory which includes large-scale data along with pre-trained model checkpoints for X-embodied robot learning research. More specifically, we provide and maintain the following open-source resources to the broader community:", "metadata": {"filetype": "application/pdf", "languages": ["eng"], "page_number": 3, "parent_id": "7e8a689d89b024a7dc47ebee59bb07fa", "filename": "rtx_paper.pdf"}}
{"type": "ListItem", "element_id": "6821f000be4d925646adb547846faa3a", "text": "\u2022 Open X-Embodiment Dataset: robot learning dataset with 1M+ robot trajectories from 22 robot embodi- ments.", "metadata": {"filetype": "application/pdf", "languages": ["eng"], "page_number": 3, "parent_id": "7e8a689d89b024a7dc47ebee59bb07fa", "filename": "rtx_paper.pdf"}}
{"type": "ListItem", "element_id": "784f76bdee32da2b616660d706a6013b", "text": "\u2022 Pre-Trained Checkpoints: a selection of RT-X model checkpoints ready for inference and finetuning.", "metadata": {"filetype": "application/pdf", "languages": ["eng"], "page_number": 3, "parent_id": "7e8a689d89b024a7dc47ebee59bb07fa", "filename": "rtx_paper.pdf"}}
{"type": "NarrativeText", "element_id": "f952515c7f81c0e370d177e1d805f163", "text": "We intend for these resources to form a foundation for X- embodiment research in robot learning, but they are just the start. Open X-Embodiment is a community-driven effort, currently involving 21 institutions from around the world, and we hope to further broaden participation and grow the initial Open X-Embodiment Dataset over time. In this sec- tion, we summarize the dataset and X-embodiment learning framework, before discussing the specific models we use to evaluate our dataset and our experimental results.", "metadata": {"filetype": "application/pdf", "languages": ["eng"], "page_number": 3, "parent_id": "7e8a689d89b024a7dc47ebee59bb07fa", "filename": "rtx_paper.pdf"}}
{"type": "NarrativeText", "element_id": "6f573bcb123b866365415202376e05b4", "text": "A. The Open X-Embodiment Dataset", "metadata": {"filetype": "application/pdf", "languages": ["eng"], "page_number": 3, "parent_id": "7e8a689d89b024a7dc47ebee59bb07fa", "filename": "rtx_paper.pdf"}}
{"type": "NarrativeText", "element_id": "d0133da536c11c63ffb8dbb729d46150", "text": "The Open X-Embodiment Dataset contains 1M+ real robot trajectories spanning 22 robot embodiments, from single robot arms to bi-manual robots and quadrupeds. The dataset was constructed by pooling 60 existing robot datasets from 34 robotic research labs around the world and converting them into a consistent data format for easy download and usage. We use the RLDS data format [119], which saves data in serialized tfrecord files and accommodates the various action spaces and input modalities of different robot setups, such as differing numbers of RGB cameras, depth cameras and point clouds. It also supports efficient, parallelized data loading in all major deep learning frameworks. For more details about the data storage format and a breakdown of all 60 datasets, see robotics-transformer-x.github.io.", "metadata": {"filetype": "application/pdf", "languages": ["eng"], "page_number": 3, "parent_id": "7e8a689d89b024a7dc47ebee59bb07fa", "filename": "rtx_paper.pdf"}}
{"type": "Title", "element_id": "482139f50b98da4165176eea35acb871", "text": "B. Dataset Analysis", "metadata": {"filetype": "application/pdf", "languages": ["eng"], "page_number": 3, "filename": "rtx_paper.pdf"}}
{"type": "NarrativeText", "element_id": "f1efa1739d41f68f49118c003cdbb87e", "text": "Fig. 2 analyzes the Open X-Embodiment Dataset. Fig. 2(a) shows the breakdown of datasets by robot embodiments, with the Franka robot being the most common. This is reflected in the number of distinct scenes (based on dataset metadata) per embodiment (Fig. 2(b)), where Franka dom- inates. Fig. 2(c) shows the breakdown of trajectories per embodiment. To further analyze the diversity, we use the language annotations present in our data. We use the PaLM language model [3] to extract objects and behaviors from", "metadata": {"filetype": "application/pdf", "languages": ["eng"], "page_number": 3, "parent_id": "482139f50b98da4165176eea35acb871", "filename": "rtx_paper.pdf"}}
{"type": "Image", "element_id": "fc4eda33f9664ad371e6720bd99f51f6", "text": "10Hz Route cable Closed Gripper B nstruction Velocity Z-Rot. Velocity images. ol aHz Pick apple from top drawer Gripper \u2018and place on counter FiLM EfficientNet ~ Transformer Position Delta Rotation Defta - Tstruction 5Hz Pick up the orange frui Image \u2014_\u2014 Gripper VIT LLM De-Tokenizer Position Delta No Rotation \"", "metadata": {"filetype": "application/pdf", "languages": ["eng"], "page_number": 4, "filename": "rtx_paper.pdf"}}
{"type": "FigureCaption", "element_id": "05ce3e18736e070f0732963799c6037e", "text": "Fig. 3: RT-1-X and RT-2-X both take images and a text instruction as input and output discretized end-effector actions. RT-1-X is an architecture designed for robotics, with a FiLM [116] conditioned EfficientNet [117] and a Transformer [118]. RT-2-X builds on a VLM backbone by representing actions as another language, and training action text tokens together with vision-language data.", "metadata": {"filetype": "application/pdf", "languages": ["eng"], "page_number": 4, "filename": "rtx_paper.pdf"}}
{"type": "NarrativeText", "element_id": "f3b2a8996d5a2a25ec281c107dadcc39", "text": "the instructions. Fig. 2(d,e) show the diversity of skills and objects. While most skills belong to the pick-place family, the long tail of the dataset contains skills like \u201cwiping\u201d or \u201cassembling\u201d. Additionally, the data covers a range of house- hold objects, from appliances to food items and utensils.", "metadata": {"filetype": "application/pdf", "languages": ["eng"], "page_number": 4, "filename": "rtx_paper.pdf"}}
{"type": "Title", "element_id": "e110774179bc4d92f2382f2063c16a2f", "text": "IV. RT-X DESIGN", "metadata": {"filetype": "application/pdf", "languages": ["eng"], "page_number": 4, "filename": "rtx_paper.pdf"}}
{"type": "NarrativeText", "element_id": "bc0bd759581aa006a8ce15beac0bc6da", "text": "To evaluate how much X-embodiment training can im- prove the performance of learned policies on individual robots, we require models that have sufficient capacity to productively make use of such large and heterogeneous datasets. To that end, our experiments will build on two recently proposed Transformer-based robotic policies: RT- 1 [8] and RT-2 [9]. We briefly summarize the design of these models in this section, and discuss how we adapted them to the X-embodiment setting in our experiments.", "metadata": {"filetype": "application/pdf", "languages": ["eng"], "page_number": 4, "parent_id": "e110774179bc4d92f2382f2063c16a2f", "filename": "rtx_paper.pdf"}}
{"type": "Title", "element_id": "6ffcbfd5e933ece63fe9d715a734707c", "text": "A. Data format consolidation", "metadata": {"filetype": "application/pdf", "languages": ["eng"], "page_number": 4, "filename": "rtx_paper.pdf"}}
{"type": "NarrativeText", "element_id": "c0a63017d2b9c7fa6cb3948f6d3701b5", "text": "One challenge of creating X-embodiment models is that observation and action spaces vary significantly across robots. We use a coarsely aligned action and observation space across datasets. The model receives a history of recent images and language instructions as observations and predicts a 7-dimensional action vector controlling the end- effector (x, y, z, roll, pitch, yaw, and gripper opening or the rates of these quantities). We select one canonical camera view from each dataset as the input image, resize it to a com- mon resolution and convert the original action set into a 7 DoF end-effector action. We normalize each dataset\u2019s actions prior to discretization. This way, an output of the model can be interpreted (de-normalized) differently depending on the embodiment used. It should be noted that despite this coarse alignment, the camera observations still vary substantially across datasets, e.g. due to differing camera poses relative to the robot or differing camera properties, see Figure 3. Similarly, for the action space, we do not align the coordinate frames across datasets in which the end-effector is controlled, and allow action values to represent either absolute or relative positions or velocities, as per the original control scheme chosen for each robot. Thus, the same action vector may induce very different motions for different robots.", "metadata": {"filetype": "application/pdf", "languages": ["eng"], "page_number": 4, "parent_id": "6ffcbfd5e933ece63fe9d715a734707c", "filename": "rtx_paper.pdf"}}
{"type": "ListItem", "element_id": "20f4ddb44f291bd52f7cd4054403a46e", "text": "B. Policy architectures", "metadata": {"filetype": "application/pdf", "languages": ["eng"], "page_number": 4, "parent_id": "6ffcbfd5e933ece63fe9d715a734707c", "filename": "rtx_paper.pdf"}}
{"type": "NarrativeText", "element_id": "fbeda62e4de4f66cc65720c2eff6e212", "text": "We consider two model architectures in our experiments: (1) RT-1 [8], an efficient Transformer-based architecture", "metadata": {"filetype": "application/pdf", "languages": ["eng"], "page_number": 4, "parent_id": "6ffcbfd5e933ece63fe9d715a734707c", "filename": "rtx_paper.pdf"}}
{"type": "NarrativeText", "element_id": "6b05bbd96087fdaa475a6f662b9f0b03", "text": "designed for robotic control, and (2) RT-2 [9] a large vision- language model co-fine-tuned to output robot actions as natural language tokens. Both models take in a visual input and natural language instruction describing the task, and output a tokenized action. For each model, the action is tokenized into 256 bins uniformly distributed along each of eight dimensions; one dimension for terminating the episode and seven dimensions for end-effector movement. Although both architectures are described in detail in their original papers [8, 9], we provide a short summary of each below:", "metadata": {"filetype": "application/pdf", "languages": ["eng"], "page_number": 4, "parent_id": "6ffcbfd5e933ece63fe9d715a734707c", "filename": "rtx_paper.pdf"}}
{"type": "NarrativeText", "element_id": "39196958a5b5e4c7c8f5b20acaf3267f", "text": "RT-1 [8] is a 35M parameter network built on a Trans- former architecture [118] and designed for robotic control, takes in a history of 15 images as shown in Fig. 3. It along with the natural language. Each image is processed through an ImageNet-pretrained EfficientNet [117] and the natural language instruction is transformed into a USE [120] embedding. The visual and language representations are then interwoven via FiLM [116] layers, producing 81 vision- language tokens. These tokens are fed into a decoder-only Transformer, which outputs the tokenized actions.", "metadata": {"filetype": "application/pdf", "languages": ["eng"], "page_number": 4, "parent_id": "6ffcbfd5e933ece63fe9d715a734707c", "filename": "rtx_paper.pdf"}}
{"type": "NarrativeText", "element_id": "27db293b4ffa784bef610bfd5755e18b", "text": "RT-2 [9] is a family of large vision-language-action models (VLAs) trained on Internet-scale vision and language data along with robotic control data. RT-2 casts the tokenized actions to text tokens, e.g., a possible action may be \u201c1 128 91 241 5 101 127\u201d. As such, any pretrained vision-language model (VLM [121\u2013123]) can be finetuned for robotic control, thus leveraging the backbone of VLMs and transferring some of their generalization properties. In this work, we focus on the RT-2-PaLI-X variant [121] built on a backbone of a visual model, ViT [124], and a language model, UL2 [125], and pretrained primarily on the WebLI [121] dataset.", "metadata": {"filetype": "application/pdf", "languages": ["eng"], "page_number": 4, "parent_id": "6ffcbfd5e933ece63fe9d715a734707c", "filename": "rtx_paper.pdf"}}
{"type": "NarrativeText", "element_id": "921fc992925e282d437be3cdb4c1326d", "text": "C. Training and inference details", "metadata": {"filetype": "application/pdf", "languages": ["eng"], "page_number": 4, "parent_id": "6ffcbfd5e933ece63fe9d715a734707c", "filename": "rtx_paper.pdf"}}
{"type": "NarrativeText", "element_id": "60e626b2c8305d448d31040dae6c6108", "text": "Both models use a standard categorical cross-entropy objective over their output space (discrete buckets for RT- 1 and all possible language tokens for RT-2).", "metadata": {"filetype": "application/pdf", "languages": ["eng"], "page_number": 4, "parent_id": "6ffcbfd5e933ece63fe9d715a734707c", "filename": "rtx_paper.pdf"}}
{"type": "NarrativeText", "element_id": "a9744358b47e754612bd04b7c0c9ec6c", "text": "We define the robotics data mixture used across all of the experiments as the data from 9 manipulators, and taken from RT-1 [8], QT-Opt [66], Bridge [95], Task Agnostic Robot Play [126, 127], Jaco Play [128], Cable Routing [129], RoboTurk [86], NYU VINN [130], Austin VIOLA [131], Berkeley Autolab UR5 [132], TOTO [133] and Language Table [91] datasets. RT-1-X is trained on only robotics mixture data defined above, whereas RT-2-X is trained via co-fine-tuning (similarly to the original RT-2 [9]), with an", "metadata": {"filetype": "application/pdf", "languages": ["eng"], "page_number": 4, "parent_id": "6ffcbfd5e933ece63fe9d715a734707c", "filename": "rtx_paper.pdf"}}
{"type": "Image", "element_id": "0b40a4ee68ddacfc2081046250150b18", "text": "iS Multi-robot & 100 RAIL 37 CvR ,\ufb01 < Multi-dataset 80 B Original Method ' \u00a9 MVP BC-RNN 60 9 Resnet + MLP 9 VINN 40 TACORL, HULC2 20 w e RT-1 =RT-1-X |s. 25. gl Kitchen Manipulation Cable Routing NYU Door Opening Autolab URS Task-Agnostic Play Mean", "metadata": {"filetype": "application/pdf", "languages": ["eng"], "page_number": 5, "filename": "rtx_paper.pdf"}}
{"type": "FigureCaption", "element_id": "8ecf3c83363de454e3dba548a5c60091", "text": "Fig. 4: RT-1-X mean success rate is 50% higher than that of either the Original Method or RT-1. RT-1 and RT-1-X have the same network architecture. Therefore the performance increase can be attributed to co-training on the robotics data mixture. The lab logos indicate the physical location of real robot evaluation, and the robot pictures indicate the embodiment used for the evaluation.", "metadata": {"filetype": "application/pdf", "languages": ["eng"], "page_number": 5, "filename": "rtx_paper.pdf"}}
{"type": "Table", "element_id": "71d0480f17aacbdc68f4b9ace1ae3547", "text": "Evaluation Setting Bridge Bridge RT-1 paper 6 skills Evaluation Location IRIS (Stanford) RAIL Lab (UCB) Google Robotic Lab Robot Embodiment Original Method WidowX LCBC [95] WidowX LCBC [95] Google Robot - Original Method RT-1 RT-1-X RT-2-X (55B) 13% 40% 27% 50% 13% 30% 27% 30% - 92% 73% 91%", "metadata": {"text_as_html": "<table><thead><tr><th>Evaluation Setting</th><th>Bridge</th><th>Bridge</th><th>RT-1 paper 6 skills</th></tr></thead><tbody><tr><td>Evaluation Location Robot Embodiment Original Method</td><td>RIS (Stanford) WidowX LCBC [95]</td><td>RAIL Lab (UCB) WidowX LCBC [95]</td><td>Google Robotic Lab Google Robot</td></tr><tr><td>Original Method</td><td>13%</td><td>13%</td><td></td></tr><tr><td>RT-1</td><td>40%</td><td>30%</td><td>9i%</td></tr><tr><td>RT-1-X</td><td>27%</td><td>27%</td><td>73%</td></tr><tr><td>RT-2-X (55B)</td><td>50%</td><td>30%</td><td>91%</td></tr></tbody></table>", "filetype": "application/pdf", "languages": ["eng"], "page_number": 5, "filename": "rtx_paper.pdf"}}
{"type": "NarrativeText", "element_id": "bb180b9fc9e8ad335b42101b48ff5c55", "text": "TABLE I: Parameter count scaling experiment to assess the impact of capacity on absorbing large-scale diverse embodiment data. For these large-scale datasets (Bridge and RT-1 paper data), RT-1-X underfits and performs worse than the Original Method and RT-1. RT-2-X model with significantly many more parameters can obtain strong performance in these two evaluation scenarios. approximately one to one split of the original VLM data and the robotics data mixture. Note that the robotics data mixture used in our experiments includes 9 embodiments which is fewer than the entire Open X-Embodiment dataset (22) \u2013 the practical reason for this difference is that we have continued to extend the dataset over time, and at the time of the experiments, the dataset above represented all of the data. In the future, we plan to continue training policies on the extended versions of the dataset as well as continue to grow the dataset together with the robot learning community. At inference time, each model is run at the rate required for the robot (3-10 Hz), with RT-1 run locally and RT-2 hosted on a cloud service and queried over the network.", "metadata": {"filetype": "application/pdf", "languages": ["eng"], "page_number": 5, "filename": "rtx_paper.pdf"}}
{"type": "NarrativeText", "element_id": "a40c09a63c8eae819ecfc36401da327f", "text": "V. EXPERIMENTAL RESULTS Our experiments answer three questions about the effect of X-embodiment training: (1) Can policies trained on our X-embodiment dataset effectively enable positive transfer, such that co-training on data collected on multiple robots improves performance on the training task? (2) Does co- training models on data from multiple platforms and tasks improve generalization to new, unseen tasks? (3) What is the influence of different design dimensions, such as model size, model architecture or dataset composition, on performance and generalization capabilities of the resulting policy? To answer these questions we conduct the total number of 3600 evaluation trials across 6 different robots.", "metadata": {"filetype": "application/pdf", "languages": ["eng"], "page_number": 5, "filename": "rtx_paper.pdf"}}
{"type": "NarrativeText", "element_id": "51875187fc077b33a9c2c4c28e120504", "text": "A. In-distribution performance across different embodiments", "metadata": {"filetype": "application/pdf", "languages": ["eng"], "page_number": 5, "filename": "rtx_paper.pdf"}}
{"type": "NarrativeText", "element_id": "28bc2890617d6f503fca4345b299df77", "text": "To assess the ability of RT-X models to learn from X- embodiment data, we evaluate performance on in-distribution tasks. We split our evaluation into two types: evaluation on domains that have small-scale datasets (Fig. 4), where we would expect transfer from larger datasets to significantly", "metadata": {"filetype": "application/pdf", "languages": ["eng"], "page_number": 5, "filename": "rtx_paper.pdf"}}
{"type": "NarrativeText", "element_id": "5aee634c1a286c5a8367bf85e696221e", "text": "improve performance, and evaluation on domains that have large-scale datasets (Table I), where we expect further im- provement to be more challenging. Note that we use the same robotics data training mixture (defined in Sec. IV-C) for all the evaluations presented in this section. For small-scale dataset experiments, we use Kitchen Manipulation [128], Cable Routing [129], NYU Door Opening [130], AUTOLab UR5 [132], and Robot Play [134]. We use the same evalua- tion and robot embodiment as in the respective publications. For large-scale dataset experiments, we consider Bridge [95] and RT-1 [8] for in-distribution evaluation and use their respective robots: WidowX and Google Robot.", "metadata": {"filetype": "application/pdf", "languages": ["eng"], "page_number": 5, "filename": "rtx_paper.pdf"}}
{"type": "NarrativeText", "element_id": "7a20e9bdbdb9542862b4dbdfbf199bf3", "text": "For each small dataset domain, we compare the perfor- mance of the RT-1-X model, and for each large dataset we consider both the RT-1-X and RT-2-X models. For all experiments, the models are co-trained on the full X- embodiment dataset. Throughout this evaluation we compare with two baseline models: (1) The model developed by the creators of the dataset trained only on that respective dataset. This constitutes a reasonable baseline insofar as it can be expected that the model has been optimized to work well with the associated data; we refer to this baseline model as the Original Method model. (2) An RT-1 model trained on the dataset in isolation; this baseline allows us to assess whether the RT-X model architectures have enough capacity to represent policies for multiple different robot platforms simultaneously, and whether co-training on multi- embodiment data leads to higher performance.", "metadata": {"filetype": "application/pdf", "languages": ["eng"], "page_number": 5, "filename": "rtx_paper.pdf"}}
{"type": "NarrativeText", "element_id": "66ea16ca3e185713490e723d9d8a2633", "text": "Small-scale dataset domains (Fig. 4). RT-1-X outper- forms Original Method trained on each of the robot-specific datasets on 4 of the 5 datasets, with a large average im- provement, demonstrating domains with limited data benefit substantially from co-training on X-embodiment data.", "metadata": {"filetype": "application/pdf", "languages": ["eng"], "page_number": 5, "filename": "rtx_paper.pdf"}}
{"type": "NarrativeText", "element_id": "b2fbca1183cc8b6527cd9527bc9f0a05", "text": "Large-scale dataset domains (Table I). In the large- dataset setting, the RT-1-X model does not outperform the RT-1 baseline trained on only the embodiment-specific dataset, which indicates underfitting for that model class. However, the larger RT-2-X model outperforms both the Original Method and RT-1 suggesting that X-robot training can improve performance in the data-rich domains, but only when utilizing a sufficiently high-capacity architecture. B. Improved generalization to out-of-distribution settings", "metadata": {"filetype": "application/pdf", "languages": ["eng"], "page_number": 5, "filename": "rtx_paper.pdf"}}
{"type": "NarrativeText", "element_id": "bbfd4674ba152b4e93126f27bfa03b03", "text": "We now examine how X-embodiment training can enable better generalization to out-of-distribution settings and more complex and novel instructions. These experiments focus on the high-data domains, and use the RT-2-X model.", "metadata": {"filetype": "application/pdf", "languages": ["eng"], "page_number": 5, "filename": "rtx_paper.pdf"}}
{"type": "Table", "element_id": "748e3d1d705272df77de2776c45e6bdd", "text": "Row Model Size History Length Dataset Co-Trained w/ Web Initial Checkpoint Emergent Skills Evaluation RT-2 Generalization Evaluation (1) 55B RT-2 (2) RT-2-X 55B (3) RT-2-X 55B (4) RT-2-X 5B (5) RT-2-X 5B (6) RT-2-X 5B (7) RT-2-X 5B none none none 2 none 2 2 Google Robot action Robotics data Robotics data except Bridge Robotics data Robotics data Robotics data Robotics data Yes Yes Yes Yes Yes No No Web-pretrained Web-pretrained Web-pretrained Web-pretrained Web-pretrained From scratch Web-pretrained 27.3% 75.8% 42.8% 44.4% 14.5% 0% 48.7% 62% 61% 54% 52% 30% 1% 47%", "metadata": {"text_as_html": "<table><thead><tr><th>Row</th><th>Model</th><th>Size</th><th>History Length</th><th>Dataset</th><th>Co-Trained w/ Web</th><th>Initial Checkpoint</th><th>Emergent Skills Evaluation</th><th>RT-2 Generalization Evaluation</th></tr></thead><tbody><tr><td>(6]</td><td>RT-2</td><td>55B</td><td>none</td><td>Google Robot action</td><td>Yes</td><td>Web-pretrained</td><td>27.3%</td><td>62%</td></tr><tr><td>)</td><td>RT-2-X</td><td>55B</td><td>none</td><td>Robotics data</td><td>Yes</td><td>Web-pretrained</td><td>75.8%</td><td>61%</td></tr><tr><td>3)</td><td>RT-2-X</td><td>55B</td><td>none</td><td>Robotics data except Bridge</td><td>Yes</td><td>Web-pretrained</td><td>42.8%</td><td>54%</td></tr><tr><td>\u201c)</td><td>RT-</td><td>5B</td><td>2</td><td>Robotics data</td><td>Yes</td><td>Web-pretrained</td><td>44.4%</td><td>52%</td></tr><tr><td>)</td><td>RT-2-</td><td>5B</td><td>none</td><td>Robotics data</td><td>Yes</td><td>Web-pretrained</td><td>14.5%</td><td>30%</td></tr><tr><td>\u00a96)</td><td>RT-2-X</td><td>5B</td><td>2</td><td>Robotics data</td><td>No</td><td>From scratch</td><td>0%</td><td>1%</td></tr><tr><td>7</td><td>RT-2-.</td><td>5B</td><td>2</td><td>Robotics data</td><td>No</td><td>Web-pretrained</td><td>48.7%</td><td>47%</td></tr></tbody></table>", "filetype": "application/pdf", "languages": ["eng"], "page_number": 6, "filename": "rtx_paper.pdf"}}
{"type": "FigureCaption", "element_id": "55ec2acc2c9a6ad37d2ac80759cf4227", "text": "TABLE II: Ablations to show the impact of design decisions on generalization (to unseen objects, backgrounds, and environments) and emergent skills (skills from other datasets on the Google Robot), showing the importance of Web-pretraining, model size, and history.", "metadata": {"filetype": "application/pdf", "languages": ["eng"], "page_number": 6, "filename": "rtx_paper.pdf"}}
{"type": "Image", "element_id": "0bade2aee1741e377fe20b28c96d4c42", "text": "importance Web-pretraining, size, (a) Absolute Motion (c) Preposition Alters Behavior move the chip bag to the put apple on cloth / top / bottom right of the counter move apple near cloth N IA@ . l\u2018 move to top right / put orange into the pot / right / bottom right move orange near pot > === | - (b) Object-Relative Motion put banana on top of the pan / move apple between move banana near pan coke and spange / cup and sponge r\u2018=\u2018 I\u2014\u2014_\u2018F\ufb01 == = A% % \"e 5: To assess transfer he[ween embodiments, we evaluate", "metadata": {"filetype": "application/pdf", "languages": ["eng"], "page_number": 6, "filename": "rtx_paper.pdf"}}
{"type": "NarrativeText", "element_id": "aa9b7e824e64072600c9208cd95eec5a", "text": "Fig. 5: To assess transfer between embodiments, we evaluate the RT-2-X model on out-of-distribution skills. These skills are in the Bridge dataset, but not in the Google Robot dataset (the embodiment they are evaluated on). VI. DISCUSSION, FUTURE WORK, AND OPEN PROBLEMS We presented a consolidated dataset that combines data from 22 robotic embodiments collected through a collab- oration between 21 institutions, demonstrating 527 skills (160266 tasks). We also presented an experimental demon- stration that Transformer-based policies trained on this data can exhibit significant positive transfer between the different robots in the dataset. Our results showed that the RT-1- X policy has a 50% higher success rate than the original, state-of-the-art methods contributed by different collabo- rating institutions, while the bigger vision-language-model- based version (RT-2-X) demonstrated \u223c 3\u00d7 generalization improvements over a model trained only on data from the evaluation embodiment. In addition, we provided multiple resources for the robotics community to explore the X- embodiment robot learning research, including: the unified X-robot and X-institution dataset, sample code showing how to use the data, and the RT-1-X model to serve as a foundation for future exploration.", "metadata": {"filetype": "application/pdf", "languages": ["eng"], "page_number": 6, "filename": "rtx_paper.pdf"}}
{"type": "NarrativeText", "element_id": "fb52c6123f40d0b394c450b53b8dabbe", "text": "While RT-X demonstrates a step towards a X-embodied robot generalist, many more steps are needed to make this future a reality. Our experiments do not consider robots with very different sensing and actuation modalities. They do not study generalization to new robots, and provide a decision criterion for when positive transfer does or does not happen. Studying these questions is an important future work direction. This work serves not only as an example that X-robot learning is feasible and practical, but also provide the tools to advance research in this direction in the future.", "metadata": {"filetype": "application/pdf", "languages": ["eng"], "page_number": 6, "filename": "rtx_paper.pdf"}}
{"type": "NarrativeText", "element_id": "a62a13c1d1079132bf3ed96e25564e02", "text": "Unseen objects, backgrounds and environments. We first conduct the same evaluation of generalization properties testing for the ability to manipulate as proposed in [9], unseen objects in unseen environments and against unseen backgrounds. We find that RT-2 and RT-2-X perform roughly on par (Table II, rows (1) and (2), last column). This is not unexpected, since RT-2 already generalizes well (see [9]) along these dimensions due to its VLM backbone.", "metadata": {"filetype": "application/pdf", "languages": ["eng"], "page_number": 6, "filename": "rtx_paper.pdf"}}
{"type": "NarrativeText", "element_id": "b02a9deff716502bd805d978728599e3", "text": "Emergent skills evaluation. To investigate the transfer of knowledge across robots, we conduct experiments with the Google Robot, assessing the performance on tasks like the ones shown in Fig. 5. These tasks involve objects and skills that are not present in the RT-2 dataset but occur in the Bridge dataset [95] for a different robot (the WidowX robot). Results are shown in Table II, Emergent Skills Evaluation column. Comparing rows (1) and (2), we find that RT-2-X outperforms RT-2 by \u223c 3\u00d7, suggesting that incorporating data from other robots into the training improves the range of tasks that can be performed even by a robot that already has large amounts of data available. Our results suggest that co-training with data from other platforms imbues the RT-2- X controller with additional skills for the platform that are not present in that platform\u2019s original dataset.", "metadata": {"filetype": "application/pdf", "languages": ["eng"], "page_number": 6, "filename": "rtx_paper.pdf"}}
{"type": "NarrativeText", "element_id": "0f718631ad988ccbf6bd5145f0f00e5b", "text": "Our next ablation involves removing the Bridge dataset from RT-2-X training: Row (3) shows the results for RT-2- X that includes all data used for RT-2-X except the Bridge dataset. This variation significantly reduces performance on the hold-out tasks, suggesting that transfer from the WidowX data may indeed be responsible for the additional skills that can be performed by RT-2-X with the Google Robot. C. Design decisions", "metadata": {"filetype": "application/pdf", "languages": ["eng"], "page_number": 6, "filename": "rtx_paper.pdf"}}
{"type": "NarrativeText", "element_id": "2ae594c3f9164b6a8077fc5dcc03a4b6", "text": "Lastly, we perform ablations to measure the influence of different design decisions on the generalization capabilities of our most performant RT-2-X model, which are presented in Table II. We note that including a short history of im- ages significantly improves generalization performance (row (4) vs row (5)). Similarly to the conclusions in the RT-2 paper [9], Web-based pre-training of the model is critical to achieving a high performance for the large models (row the 55B model has (4) vs row (6)). We also note that significantly higher success rate in the Emergent Skills com- pared to the 5B model (row (2) vs row (4)), demonstrating that higher model capacity enables higher degree of transfer across robotic datasets. Contrary to previous RT-2 findings, co-fine-tuning and fine-tuning have similar performance in both the Emergent Skills and Generalization Evaluation (row (4) vs row (7)), which we attribute to the fact that the robotics data used in RT-2-X is much more diverse than the previously used robotics datasets.", "metadata": {"filetype": "application/pdf", "languages": ["eng"], "page_number": 6, "filename": "rtx_paper.pdf"}}
{"type": "Title", "element_id": "0200b826f862b4f75389fd989b474835", "text": "REFERENCES", "metadata": {"filetype": "application/pdf", "languages": ["eng"], "page_number": 7, "filename": "rtx_paper.pdf"}}
{"type": "ListItem", "element_id": "0fa009852f42756c59cfb99a1735e577", "text": "[1] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark et al., \u201cLearning transferable visual models from natural language supervision,\u201d in International conference on machine learning. PMLR, 2021, pp. 8748\u20138763.", "metadata": {"filetype": "application/pdf", "languages": ["eng"], "page_number": 7, "parent_id": "0200b826f862b4f75389fd989b474835", "filename": "rtx_paper.pdf"}}
{"type": "ListItem", "element_id": "90a258f44ab4927279600617c42a2ff3", "text": "[2] OpenAI, \u201cGPT-4 technical report,\u201d 2023. [3] R. Anil, A. M. Dai, O. Firat, M. Johnson, D. Lepikhin, A. Passos, S. Shakeri, E. Taropa, P. Bailey, Z. Chen et al., \u201cPaLM 2 technical report,\u201d arXiv preprint arXiv:2305.10403, 2023.", "metadata": {"filetype": "application/pdf", "languages": ["eng"], "page_number": 7, "parent_id": "0200b826f862b4f75389fd989b474835", "filename": "rtx_paper.pdf"}}
{"type": "ListItem", "element_id": "c0171cbe273ed7642127d890b059afec", "text": "[4] T. Weyand, A. Araujo, B. Cao, and J. Sim, \u201cGoogle landmarks dataset v2 - a large-scale benchmark for instance-level recognition and retrieval,\u201d in Proceed- ings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2020.", "metadata": {"filetype": "application/pdf", "languages": ["eng"], "page_number": 7, "parent_id": "0200b826f862b4f75389fd989b474835", "filename": "rtx_paper.pdf"}}
{"type": "ListItem", "element_id": "8ac159c58051018c2aa7c55cdf4332bf", "text": "[5] B. Wu, W. Chen, Y. Fan, Y. Zhang, J. Hou, J. Liu, and T. Zhang, \u201cTencent ML-images: A large-scale multi-label image database for visual representation learning,\u201d IEEE Access, vol. 7, 2019.", "metadata": {"filetype": "application/pdf", "languages": ["eng"], "page_number": 7, "parent_id": "0200b826f862b4f75389fd989b474835", "filename": "rtx_paper.pdf"}}
{"type": "ListItem", "element_id": "8ece0b0ea1aace429ca085aa66d497e2", "text": "Jentzsch, D. Kontokostas, P. N. Mendes, S. Hellmann, M. Morsey, P. van Kleef, S. Auer, and C. Bizer, \u201cDBpedia - a large-scale, multilingual knowledge base extracted from wikipedia.\u201d Semantic Web, [Online]. vol. 6, no. 2, pp. 167\u2013195, 2015. Available: http://dblp.uni-trier.de/db/journals/semweb/ semweb6.html#LehmannIJJKMHMK15", "metadata": {"filetype": "application/pdf", "languages": ["eng"], "page_number": 7, "parent_id": "0200b826f862b4f75389fd989b474835", "filename": "rtx_paper.pdf"}}
{"type": "ListItem", "element_id": "d11c0fb4855ff5050f229134688e5a8b", "text": "[7] H. M\u00a8uhleisen and C. Bizer, \u201cWeb data commons- extracting structured data from two large web cor- pora.\u201d LDOW, vol. 937, pp. 133\u2013145, 2012.", "metadata": {"filetype": "application/pdf", "languages": ["eng"], "page_number": 7, "parent_id": "0200b826f862b4f75389fd989b474835", "filename": "rtx_paper.pdf"}}
{"type": "ListItem", "element_id": "5aea48db1d102b88a6ff7f98a00b0044", "text": "[8] A. Brohan, N. Brown, J. Carbajal, Y. Chebotar, J. Dabis, C. Finn, K. Gopalakrishnan, K. Hausman, A. Herzog, J. Hsu et al., \u201cRT-1: Robotics transformer for real-world control at scale,\u201d Robotics: Science and Systems (RSS), 2023.", "metadata": {"filetype": "application/pdf", "languages": ["eng"], "page_number": 7, "parent_id": "0200b826f862b4f75389fd989b474835", "filename": "rtx_paper.pdf"}}
{"type": "ListItem", "element_id": "9777c4e5a80e4bee1d971a24a6f821ac", "text": "[9] A. Brohan, N. Brown, J. Carbajal, Y. Chebotar, X. Chen, K. Choromanski, T. Ding, D. Driess, A. Dubey, C. Finn et al., \u201cRT-2: Vision-language- action models transfer web knowledge to robotic con- trol,\u201d arXiv preprint arXiv:2307.15818, 2023.", "metadata": {"filetype": "application/pdf", "languages": ["eng"], "page_number": 7, "parent_id": "0200b826f862b4f75389fd989b474835", "filename": "rtx_paper.pdf"}}
{"type": "ListItem", "element_id": "0ee32165b15ffc03d9a7811e5a70e5c1", "text": "[10] C. Devin, A. Gupta, T. Darrell, P. Abbeel, and S. Levine, \u201cLearning modular neural network policies for multi-task and multi-robot transfer,\u201d in 2017 IEEE international conference on robotics and automation (ICRA).", "metadata": {"filetype": "application/pdf", "languages": ["eng"], "page_number": 7, "parent_id": "0200b826f862b4f75389fd989b474835", "filename": "rtx_paper.pdf"}}
{"type": "ListItem", "element_id": "fcf4134ebb8df5805a8685eb30f27324", "text": "IEEE, 2017, pp. 2169\u20132176. [11] T. Chen, A. Murali, and A. Gupta, \u201cHardware con- ditioned policies for multi-robot transfer learning,\u201d in Advances in Neural Information Processing Systems, 2018, pp. 9355\u20139366.", "metadata": {"filetype": "application/pdf", "languages": ["eng"], "page_number": 7, "parent_id": "0200b826f862b4f75389fd989b474835", "filename": "rtx_paper.pdf"}}
{"type": "ListItem", "element_id": "a207797d65e70bb9e28dc89acad76e01", "text": "[12] A. Sanchez-Gonzalez, N. Heess, J. T. Springenberg, J. Merel, M. Riedmiller, R. Hadsell, and P. Battaglia, \u201cGraph networks as learnable physics engines for inference and control,\u201d in Proceedings of the 35th International Conference on Machine Learning, ser.", "metadata": {"filetype": "application/pdf", "languages": ["eng"], "page_number": 7, "parent_id": "0200b826f862b4f75389fd989b474835", "filename": "rtx_paper.pdf"}}
{"type": "NarrativeText", "element_id": "2701a1990bcecb9b74f83ce34bfde0a4", "text": "Proceedings of Machine Learning Research, J. Dy and A. Krause, Eds., vol. 80. PMLR, 10\u201315 Jul 2018, pp. 4470\u20134479. [Online]. Available: https:// proceedings.mlr.press/v80/sanchez-gonzalez18a.html [13] D. Pathak, C. Lu, T. Darrell, P. Isola, and A. A. Efros, \u201cLearning to control self-assembling morphologies: a study of generalization via modularity,\u201d Advances in Neural Information Processing Systems, vol. 32, 2019. [14] R. Mart\u00b4\u0131n-Mart\u00b4\u0131n, M. Lee, R. Gardner, S. Savarese, J. Bohg, and A. Garg, \u201cVariable impedance control in end-effector space. an action space for reinforcement learning in contact rich tasks,\u201d in Proceedings of the International Conference of Intelligent Robots and Systems (IROS), 2019.", "metadata": {"filetype": "application/pdf", "languages": ["eng"], "page_number": 7, "parent_id": "0200b826f862b4f75389fd989b474835", "filename": "rtx_paper.pdf"}}
{"type": "ListItem", "element_id": "a12509bc88ba7fc8ef54870252dec897", "text": "[15] W. Huang, I. Mordatch, and D. Pathak, \u201cOne policy to control them all: Shared modular policies for agent- agnostic control,\u201d in ICML, 2020.", "metadata": {"filetype": "application/pdf", "languages": ["eng"], "page_number": 7, "parent_id": "0200b826f862b4f75389fd989b474835", "filename": "rtx_paper.pdf"}}
{"type": "ListItem", "element_id": "36085658a2638e26f5d6803d8be7759f", "text": "[16] V. Kurin, M. Igl, T. Rockt\u00a8aschel, W. Boehmer, and S. Whiteson, \u201cMy body is a cage: the role of mor- phology in graph-based incompatible control,\u201d arXiv preprint arXiv:2010.01856, 2020.", "metadata": {"filetype": "application/pdf", "languages": ["eng"], "page_number": 7, "parent_id": "0200b826f862b4f75389fd989b474835", "filename": "rtx_paper.pdf"}}
{"type": "ListItem", "element_id": "9ee7b72ffefb6956ab496f389a3a9389", "text": "[17] K. Zakka, A. Zeng, P. Florence, J. Tompson, J. Bohg, and D. Dwibedi, \u201cXIRL: Cross-embodiment inverse reinforcement learning,\u201d Conference on Robot Learn- ing (CoRL), 2021.", "metadata": {"filetype": "application/pdf", "languages": ["eng"], "page_number": 7, "parent_id": "0200b826f862b4f75389fd989b474835", "filename": "rtx_paper.pdf"}}
{"type": "ListItem", "element_id": "df9c87a54ce1ef09c0828a27d393459d", "text": "[18] A. Ghadirzadeh, X. Chen, P. Poklukar, C. Finn, M. Bj\u00a8orkman, and D. Kragic, \u201cBayesian meta-learning for few-shot policy adaptation across robotic plat- forms,\u201d in 2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE, 2021, pp. 1274\u20131280.", "metadata": {"filetype": "application/pdf", "languages": ["eng"], "page_number": 7, "parent_id": "0200b826f862b4f75389fd989b474835", "filename": "rtx_paper.pdf"}}
{"type": "ListItem", "element_id": "15a6afb4ba8f05a6c709a7a5cbf487b5", "text": "[19] A. Gupta, L. Fan, S. Ganguli, and L. Fei-Fei, \u201cMeta- morph: Learning universal controllers with transform- ers,\u201d in International Conference on Learning Repre- sentations, 2021.", "metadata": {"filetype": "application/pdf", "languages": ["eng"], "page_number": 7, "parent_id": "0200b826f862b4f75389fd989b474835", "filename": "rtx_paper.pdf"}}
{"type": "ListItem", "element_id": "4d86a4157607753cc7b1eb6441db694b", "text": "J. Bruce, S. Bechtle, E. Parisotto, M. Riedmiller, J. T. Springenberg, A. Byravan, L. Hasenclever, and N. Heess, \u201cA gen- eralist dynamics model for control,\u201d 2023.", "metadata": {"filetype": "application/pdf", "languages": ["eng"], "page_number": 7, "parent_id": "0200b826f862b4f75389fd989b474835", "filename": "rtx_paper.pdf"}}
{"type": "ListItem", "element_id": "2410c94b7b293ca20ba13e35ff345ba1", "text": "[21] D. Shah, A. Sridhar, A. Bhorkar, N. Hirose, and S. Levine, \u201cGNM: A general navigation model to drive any robot,\u201d in 2023 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2023, pp. 7226\u20137233.", "metadata": {"filetype": "application/pdf", "languages": ["eng"], "page_number": 7, "parent_id": "0200b826f862b4f75389fd989b474835", "filename": "rtx_paper.pdf"}}
{"type": "NarrativeText", "element_id": "6469a562308eac2177f0c53b765f7f0a", "text": "[22] Y. Zhou, S. Sonawani, M. Phielipp, S. Stepputtis, and H. Amor, \u201cModularity through attention: Efficient training and transfer of language-conditioned policies for robot manipulation,\u201d in Proceedings of The 6th Conference on Robot Learning, ser. Proceedings of Machine Learning Research, K. Liu, D. Kulic, and J. PMLR, 14\u2013 18 Dec 2023, pp. 1684\u20131695. [Online]. Available: https://proceedings.mlr.press/v205/zhou23b.html [23] S. Dasari, F. Ebert, S. Tian, S. Nair, B. Bucher, K. Schmeckpeper, S. Singh, S. Levine, and C. Finn, \u201cRoboNet: Large-scale multi-robot learning,\u201d in Con- ference on Robot Learning (CoRL), vol. 100. PMLR,", "metadata": {"filetype": "application/pdf", "languages": ["eng"], "page_number": 7, "parent_id": "0200b826f862b4f75389fd989b474835", "filename": "rtx_paper.pdf"}}
{"type": "NarrativeText", "element_id": "53b6537ed56d1e596374bf5b2589e5c1", "text": "2019, pp. 885\u2013897.", "metadata": {"filetype": "application/pdf", "languages": ["eng"], "page_number": 8, "parent_id": "0200b826f862b4f75389fd989b474835", "filename": "rtx_paper.pdf"}}
{"type": "ListItem", "element_id": "6072ccc6efed9d401f08f2187197e0f5", "text": "[24] E. S. Hu, K. Huang, O. Rybkin, and D. Jayaraman, \u201cKnow thyself: Transferable visual control policies through robot-awareness,\u201d in International Conference on Learning Representations, 2022.", "metadata": {"filetype": "application/pdf", "languages": ["eng"], "page_number": 8, "parent_id": "0200b826f862b4f75389fd989b474835", "filename": "rtx_paper.pdf"}}
{"type": "ListItem", "element_id": "ed859e583569cd67bcc2354d43738add", "text": "[25] K. Bousmalis, G. Vezzani, D. Rao, C. Devin, A. X. Lee, M. Bauza, T. Davchev, Y. Zhou, A. Gupta, A. Raju et al., \u201cRoboCat: A self-improving founda- tion agent for robotic manipulation,\u201d arXiv preprint arXiv:2306.11706, 2023.", "metadata": {"filetype": "application/pdf", "languages": ["eng"], "page_number": 8, "parent_id": "0200b826f862b4f75389fd989b474835", "filename": "rtx_paper.pdf"}}
{"type": "ListItem", "element_id": "45305b27238d99d4b78e084b6b300eb0", "text": "[26] J. Yang, D. Sadigh, and C. Finn, \u201cPolybot: Training one policy across robots while embracing variability,\u201d arXiv preprint arXiv:2307.03719, 2023.", "metadata": {"filetype": "application/pdf", "languages": ["eng"], "page_number": 8, "parent_id": "0200b826f862b4f75389fd989b474835", "filename": "rtx_paper.pdf"}}
{"type": "ListItem", "element_id": "edfbd57d1d30352d76443c7d841abee2", "text": "[27] S. Reed, K. Zolna, E. Parisotto, S. G. Colmenarejo, A. Novikov, G. Barth-maron, M. Gim\u00b4enez, Y. Sul- sky, J. Kay, J. T. Springenberg, T. Eccles, J. Bruce, A. Razavi, A. Edwards, N. Heess, Y. Chen, R. Had- sell, O. Vinyals, M. Bordbar, and N. de Freitas, \u201cA generalist agent,\u201d Transactions on Machine Learning Research, 2022.", "metadata": {"filetype": "application/pdf", "languages": ["eng"], "page_number": 8, "parent_id": "0200b826f862b4f75389fd989b474835", "filename": "rtx_paper.pdf"}}
{"type": "NarrativeText", "element_id": "21cbd0a4666e455e00262ab40c0a6135", "text": "[28] G. Salhotra, I.-C. A. Liu, and G. Sukhatme, \u201cBridging action space mismatch in learning from demonstra- tions,\u201d arXiv preprint arXiv:2304.03833, 2023. [29] I. Radosavovic, B. Shi, L. Fu, K. Goldberg, T. Darrell, and J. Malik, \u201cRobot learning with sensorimotor pre- training,\u201d in Conference on Robot Learning, 2023. [30] L. Shao, F. Ferreira, M. Jorda, V. Nambiar, J. Luo, E. Solowjow, J. A. Ojea, O. Khatib, and J. Bohg, \u201cUniGrasp: Learning a unified model to grasp with multifingered robotic hands,\u201d IEEE Robotics and Au- tomation Letters, vol. 5, no. 2, pp. 2286\u20132293, 2020. [31] Z. Xu, B. Qi, S. Agrawal, and S. Song, \u201cAdagrasp: Learning an adaptive gripper-aware grasping policy,\u201d in 2021 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2021, pp. 4620\u20134626. [32] D. Shah, A. Sridhar, N. Dashora, K. Stachowicz, K. Black, N. Hirose, and S. Levine, \u201cViNT: A Foun- dation Model for Visual Navigation,\u201d in 7th Annual Conference on Robot Learning (CoRL), 2023. [33] Y. Liu, A. Gupta, P. Abbeel, and S. Levine, \u201cImitation from observation: Learning to imitate behaviors from translation,\u201d in 2018 IEEE raw video via context International Conference on Robotics and Automation (ICRA).", "metadata": {"filetype": "application/pdf", "languages": ["eng"], "page_number": 8, "parent_id": "0200b826f862b4f75389fd989b474835", "filename": "rtx_paper.pdf"}}
{"type": "ListItem", "element_id": "1a71c4c458e446e164d7be1e52620e69", "text": "IEEE, 2018, pp. 1118\u20131125. [34] T. Yu, C. Finn, S. Dasari, A. Xie, T. Zhang, P. Abbeel, and S. Levine, \u201cOne-shot imitation from observing hu- mans via domain-adaptive meta-learning,\u201d Robotics: Science and Systems XIV, 2018.", "metadata": {"filetype": "application/pdf", "languages": ["eng"], "page_number": 8, "parent_id": "0200b826f862b4f75389fd989b474835", "filename": "rtx_paper.pdf"}}
{"type": "ListItem", "element_id": "d9c8053d666ce2a062e0d3787c0d2fad", "text": "[35] P. Sharma, D. Pathak, and A. Gupta, \u201cThird-person visual imitation learning via decoupled hierarchical controller,\u201d Advances in Neural Information Process- ing Systems, vol. 32, 2019.", "metadata": {"filetype": "application/pdf", "languages": ["eng"], "page_number": 8, "parent_id": "0200b826f862b4f75389fd989b474835", "filename": "rtx_paper.pdf"}}
{"type": "ListItem", "element_id": "4022dc8c40abac1a28aa26efac985808", "text": "[36] L. Smith, N. Dhawan, M. Zhang, P. Abbeel, and S. Levine, \u201cAvid: Learning multi-stage tasks via pixel- translation of human videos,\u201d arXiv preprint level arXiv:1912.04443, 2019.", "metadata": {"filetype": "application/pdf", "languages": ["eng"], "page_number": 8, "parent_id": "0200b826f862b4f75389fd989b474835", "filename": "rtx_paper.pdf"}}
{"type": "ListItem", "element_id": "12c72a79e80dfab733f3c4b452985326", "text": "[37] A. Bonardi, S. James, and A. J. Davison, \u201cLearning", "metadata": {"filetype": "application/pdf", "languages": ["eng"], "page_number": 8, "parent_id": "0200b826f862b4f75389fd989b474835", "filename": "rtx_paper.pdf"}}
{"type": "NarrativeText", "element_id": "019f838fdd58122bcb071a8ca8bf2021", "text": "one-shot imitation from humans without humans,\u201d IEEE Robotics and Automation Letters, vol. 5, no. 2, pp. 3533\u20133539, 2020.", "metadata": {"filetype": "application/pdf", "languages": ["eng"], "page_number": 8, "parent_id": "0200b826f862b4f75389fd989b474835", "filename": "rtx_paper.pdf"}}
{"type": "ListItem", "element_id": "768e604e1b1fa1ba12a9a15cef0fefdf", "text": "[38] K. Schmeckpeper, O. Rybkin, K. Daniilidis, S. Levine, and C. Finn, \u201cReinforcement learning with videos: Combining offline observations with interaction,\u201d in Conference on Robot Learning. PMLR, 2021, pp. 339\u2013354.", "metadata": {"filetype": "application/pdf", "languages": ["eng"], "page_number": 8, "parent_id": "0200b826f862b4f75389fd989b474835", "filename": "rtx_paper.pdf"}}
{"type": "ListItem", "element_id": "8c60ea2f41874bd2df19c2ce9d6075f5", "text": "[39] H. Xiong, Q. Li, Y.-C. Chen, H. Bharadhwaj, S. Sinha, and A. Garg, \u201cLearning by watching: Physical imita- tion of manipulation skills from human videos,\u201d in 2021 IEEE/RSJ International Conference on Intelli- gent Robots and Systems (IROS). IEEE, 2021, pp. 7827\u20137834.", "metadata": {"filetype": "application/pdf", "languages": ["eng"], "page_number": 8, "parent_id": "0200b826f862b4f75389fd989b474835", "filename": "rtx_paper.pdf"}}
{"type": "ListItem", "element_id": "abfa0dfcb114c28c51150ad2a93226d6", "text": "[40] E. Jang, A. Irpan, M. Khansari, D. Kappler, F. Ebert, C. Lynch, S. Levine, and C. Finn, \u201cBC-Z: Zero-shot task generalization with robotic imitation learning,\u201d in Conference on Robot Learning (CoRL), 2021, pp. 991\u20131002.", "metadata": {"filetype": "application/pdf", "languages": ["eng"], "page_number": 8, "parent_id": "0200b826f862b4f75389fd989b474835", "filename": "rtx_paper.pdf"}}
{"type": "ListItem", "element_id": "4ebc8594d6e498432a9bdaa5cb961500", "text": "[41] S. Bahl, A. Gupta, and D. Pathak, \u201cHuman-to-robot imitation in the wild,\u201d Robotics: Science and Systems (RSS), 2022.", "metadata": {"filetype": "application/pdf", "languages": ["eng"], "page_number": 8, "parent_id": "0200b826f862b4f75389fd989b474835", "filename": "rtx_paper.pdf"}}
{"type": "NarrativeText", "element_id": "2d0c9b0d748148deb7afd7a93cce87b3", "text": "[42] M. Ding, Y. Xu, Z. Chen, D. D. Cox, P. Luo, J. B. Tenenbaum, and C. Gan, \u201cEmbodied concept learner: Self-supervised learning of concepts and map- ping through instruction following,\u201d in Conference on Robot Learning. PMLR, 2023, pp. 1743\u20131754. [43] S. Bahl, R. Mendonca, L. Chen, U. Jain, and D. Pathak, \u201cAffordances from human videos as a versatile representation for robotics,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2023, pp. 13 778\u2013 13 790.", "metadata": {"filetype": "application/pdf", "languages": ["eng"], "page_number": 8, "parent_id": "0200b826f862b4f75389fd989b474835", "filename": "rtx_paper.pdf"}}
{"type": "ListItem", "element_id": "cf277681269d5566317900c9ae5f2e3a", "text": "[44] P. Sermanet, K. Xu, and S. Levine, \u201cUnsupervised per- ceptual rewards for imitation learning,\u201d arXiv preprint arXiv:1612.06699, 2016.", "metadata": {"filetype": "application/pdf", "languages": ["eng"], "page_number": 8, "parent_id": "0200b826f862b4f75389fd989b474835", "filename": "rtx_paper.pdf"}}
{"type": "ListItem", "element_id": "59187b613324b5730a46f5cab245a53a", "text": "[45] L. Shao, T. Migimatsu, Q. Zhang, K. Yang, and J. Bohg, \u201cConcept2Robot: Learning manipulation con- cepts from instructions and human demonstrations,\u201d in Proceedings of Robotics: Science and Systems (RSS), 2020.", "metadata": {"filetype": "application/pdf", "languages": ["eng"], "page_number": 8, "parent_id": "0200b826f862b4f75389fd989b474835", "filename": "rtx_paper.pdf"}}
{"type": "NarrativeText", "element_id": "dc76014627a55828e976a0e8b8e0f0b1", "text": "[46] A. S. Chen, S. Nair, and C. Finn, \u201cLearning generaliz- able robotic reward functions from \u201cin-the-wild\u201d hu- man videos,\u201d arXiv preprint arXiv:2103.16817, 2021. [47] S. Kumar, J. Zamora, N. Hansen, R. Jangir, and X. Wang, \u201cGraph inverse reinforcement learning from diverse videos,\u201d in Conference on Robot Learning. PMLR, 2023, pp. 55\u201366.", "metadata": {"filetype": "application/pdf", "languages": ["eng"], "page_number": 8, "parent_id": "0200b826f862b4f75389fd989b474835", "filename": "rtx_paper.pdf"}}
{"type": "ListItem", "element_id": "ae19c67ba743ab9a60f6c728a374905d", "text": "[48] M. Alakuijala, G. Dulac-Arnold, J. Mairal, J. Ponce, and C. Schmid, \u201cLearning reward functions for robotic manipulation by observing humans,\u201d in 2023 IEEE International Conference on Robotics and Automation (ICRA).", "metadata": {"filetype": "application/pdf", "languages": ["eng"], "page_number": 8, "parent_id": "0200b826f862b4f75389fd989b474835", "filename": "rtx_paper.pdf"}}
{"type": "NarrativeText", "element_id": "fa4156d12e67bedb92a314eea5e2e179", "text": "IEEE, 2023, pp. 5006\u20135012. [49] Y. Zhou, Y. Aytar, and K. Bousmalis, \u201cManipulator- imitation,\u201d", "metadata": {"filetype": "application/pdf", "languages": ["eng"], "page_number": 8, "parent_id": "0200b826f862b4f75389fd989b474835", "filename": "rtx_paper.pdf"}}
{"type": "ListItem", "element_id": "1e505ed7f2c5bb54643ec8a94d6b9941", "text": "independent 2021. representations for visual", "metadata": {"filetype": "application/pdf", "languages": ["eng"], "page_number": 8, "parent_id": "0200b826f862b4f75389fd989b474835", "filename": "rtx_paper.pdf"}}
{"type": "ListItem", "element_id": "0f4fc04cf6ba704dee0cc85bd778f856", "text": "[50] C. Wang, L. Fan, J. Sun, R. Zhang, L. Fei-Fei, D. Xu,", "metadata": {"filetype": "application/pdf", "languages": ["eng"], "page_number": 8, "parent_id": "0200b826f862b4f75389fd989b474835", "filename": "rtx_paper.pdf"}}
{"type": "NarrativeText", "element_id": "c9211010d75fea026a372f62b7e9530c", "text": "Y. Zhu, and A. Anandkumar, \u201cMimicplay: Long- horizon imitation learning by watching human play,\u201d in Conference on Robot Learning, 2023.", "metadata": {"filetype": "application/pdf", "languages": ["eng"], "page_number": 9, "parent_id": "0200b826f862b4f75389fd989b474835", "filename": "rtx_paper.pdf"}}
{"type": "ListItem", "element_id": "60a8b9ee17fe13cbb71cc719a6e68812", "text": "[51] K. Schmeckpeper, A. Xie, O. Rybkin, S. Tian, K. Daniilidis, S. Levine, and C. Finn, \u201cLearning pre- dictive models from observation and interaction,\u201d in European Conference on Computer Vision. Springer, 2020, pp. 708\u2013725.", "metadata": {"filetype": "application/pdf", "languages": ["eng"], "page_number": 9, "parent_id": "0200b826f862b4f75389fd989b474835", "filename": "rtx_paper.pdf"}}
{"type": "ListItem", "element_id": "c1244751ddccb18ec36935e2901c6c89", "text": "[52] S. Nair, A. Rajeswaran, V. Kumar, C. Finn, and A. Gupta, \u201cR3m: A universal visual representation for robot manipulation,\u201d in CoRL, 2022.", "metadata": {"filetype": "application/pdf", "languages": ["eng"], "page_number": 9, "parent_id": "0200b826f862b4f75389fd989b474835", "filename": "rtx_paper.pdf"}}
{"type": "ListItem", "element_id": "cc185e7981b084aef0eaedc11d21fbea", "text": "[53] T. Xiao, I. Radosavovic, T. Darrell, and J. Malik, \u201cMasked visual pre-training for motor control,\u201d arXiv preprint arXiv:2203.06173, 2022.", "metadata": {"filetype": "application/pdf", "languages": ["eng"], "page_number": 9, "parent_id": "0200b826f862b4f75389fd989b474835", "filename": "rtx_paper.pdf"}}
{"type": "ListItem", "element_id": "5d85b6dce93bcb3ff8a9f4495d77fd6f", "text": "[54] I. Radosavovic, T. Xiao, S. James, P. Abbeel, J. Ma- lik, and T. Darrell, \u201cReal-world robot learning with masked visual pre-training,\u201d in Conference on Robot Learning, 2022.", "metadata": {"filetype": "application/pdf", "languages": ["eng"], "page_number": 9, "parent_id": "0200b826f862b4f75389fd989b474835", "filename": "rtx_paper.pdf"}}
{"type": "NarrativeText", "element_id": "82df9cea0469ce315543d86229c049fa", "text": "[55] Y. J. Ma, S. Sodhani, D. Jayaraman, O. Bastani, V. Kumar, and A. Zhang, \u201cVip: Towards universal visual reward and representation via value-implicit pre-training,\u201d arXiv preprint arXiv:2210.00030, 2022. [56] A. Majumdar, K. Yadav, S. Arnaud, Y. J. Ma, C. Chen, S. Silwal, A. Jain, V.-P. Berges, P. Abbeel, J. Malik et al., \u201cWhere are we in the search for an artificial vi- sual cortex for embodied intelligence?\u201d arXiv preprint arXiv:2303.18240, 2023.", "metadata": {"filetype": "application/pdf", "languages": ["eng"], "page_number": 9, "parent_id": "0200b826f862b4f75389fd989b474835", "filename": "rtx_paper.pdf"}}
{"type": "ListItem", "element_id": "11b488be49aba4a5a2407486efc33c2e", "text": "[57] S. Karamcheti, S. Nair, A. S. Chen, T. Kollar, C. Finn, D. Sadigh, and P. Liang, \u201cLanguage-driven represen- tation learning for robotics,\u201d Robotics: Science and Systems (RSS), 2023.", "metadata": {"filetype": "application/pdf", "languages": ["eng"], "page_number": 9, "parent_id": "0200b826f862b4f75389fd989b474835", "filename": "rtx_paper.pdf"}}
{"type": "ListItem", "element_id": "7af09d2caec1a2363471befeb615b472", "text": "[58] Y. Mu, S. Yao, M. Ding, P. Luo, and C. Gan, \u201cEC2: Emergent communication for embodied control,\u201d in Proceedings of the IEEE/CVF Conference on Com- puter Vision and Pattern Recognition, 2023, pp. 6704\u2013 6714.", "metadata": {"filetype": "application/pdf", "languages": ["eng"], "page_number": 9, "parent_id": "0200b826f862b4f75389fd989b474835", "filename": "rtx_paper.pdf"}}
{"type": "NarrativeText", "element_id": "a32fb6013c05878487d0d1b9529dd852", "text": "[59] S. Bahl, R. Mendonca, L. Chen, U. Jain, and D. Pathak, \u201cAffordances from human videos as a versatile representation for robotics,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023, pp. 13 778\u201313 790. [60] Y. Jiang, S. Moseson, and A. Saxena, \u201cEfficient grasping from RGBD images: Learning using a new rectangle representation,\u201d in 2011 IEEE International conference on robotics and automation. IEEE, 2011, pp. 3304\u20133311.", "metadata": {"filetype": "application/pdf", "languages": ["eng"], "page_number": 9, "parent_id": "0200b826f862b4f75389fd989b474835", "filename": "rtx_paper.pdf"}}
{"type": "ListItem", "element_id": "0b6f6ab14e338c603cb6a5e14a023024", "text": "[61] L. Pinto and A. K. Gupta, \u201cSupersizing self- supervision: Learning to grasp from 50k tries and 700 robot hours,\u201d 2016 IEEE International Conference on Robotics and Automation (ICRA), pp. 3406\u20133413, 2015.", "metadata": {"filetype": "application/pdf", "languages": ["eng"], "page_number": 9, "parent_id": "0200b826f862b4f75389fd989b474835", "filename": "rtx_paper.pdf"}}
{"type": "ListItem", "element_id": "34f61be252b78c5d13433bf8316a58e5", "text": "[62] D. Kappler, J. Bohg, and S. Schaal, \u201cLeveraging big data for grasp planning,\u201d in ICRA, 2015, pp. 4304\u2013 4311.", "metadata": {"filetype": "application/pdf", "languages": ["eng"], "page_number": 9, "parent_id": "0200b826f862b4f75389fd989b474835", "filename": "rtx_paper.pdf"}}
{"type": "ListItem", "element_id": "4fcd2d77eef97792935547eecb0fbd18", "text": "[63] J. Mahler, J. Liang, S. Niyaz, M. Laskey, R. Doan, X. Liu, J. A. Ojea, and K. Goldberg, \u201cDex-Net 2.0: Deep learning to plan robust grasps with synthetic", "metadata": {"filetype": "application/pdf", "languages": ["eng"], "page_number": 9, "parent_id": "0200b826f862b4f75389fd989b474835", "filename": "rtx_paper.pdf"}}
{"type": "NarrativeText", "element_id": "0735ecfab6ee0ebeedc3bd1d435b536f", "text": "point clouds and analytic grasp metrics,\u201d in Robotics: Science and Systems (RSS), 2017.", "metadata": {"filetype": "application/pdf", "languages": ["eng"], "page_number": 9, "parent_id": "0200b826f862b4f75389fd989b474835", "filename": "rtx_paper.pdf"}}
{"type": "ListItem", "element_id": "358aab5a66aac9090e014194795f20b9", "text": "[64] A. Depierre, E. Dellandr\u00b4ea, and L. Chen, \u201cJacquard: A large scale dataset for robotic grasp detection,\u201d in 2018 IEEE/RSJ International Conference on Intelli- gent Robots and Systems (IROS). IEEE, 2018, pp. 3511\u20133516.", "metadata": {"filetype": "application/pdf", "languages": ["eng"], "page_number": 9, "parent_id": "0200b826f862b4f75389fd989b474835", "filename": "rtx_paper.pdf"}}
{"type": "NarrativeText", "element_id": "7acf6327a229c7876e50125730e1f966", "text": "[65] S. Levine, P. Pastor, A. Krizhevsky, J. Ibarz, and D. Quillen, \u201cLearning hand-eye coordination for robotic grasping with deep learning and large-scale data collection,\u201d The International journal of robotics research, vol. 37, no. 4-5, pp. 421\u2013436, 2018. [66] D. Kalashnikov, A. Irpan, P. Pastor, J. Ibarz, A. Her- zog, E. Jang, D. Quillen, E. Holly, M. Kalakrishnan, V. Vanhoucke et al., \u201cQT-Opt: Scalable deep rein- forcement learning for vision-based robotic manipu- lation,\u201d arXiv preprint arXiv:1806.10293, 2018. [67] S. Brahmbhatt, C. Ham, C. Kemp, and J. Hays, \u201cContactdb: Analyzing and predicting grasp contact via thermal imaging,\u201d 04 2019.", "metadata": {"filetype": "application/pdf", "languages": ["eng"], "page_number": 9, "parent_id": "0200b826f862b4f75389fd989b474835", "filename": "rtx_paper.pdf"}}
{"type": "ListItem", "element_id": "6c9c8a3b23fc5aa9a7fbb46e0119d364", "text": "[68] H.-S. Fang, C. Wang, M. Gou, and C. Lu, \u201cGraspnet- 1billion: a large-scale benchmark for general object grasping,\u201d in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2020, pp. 11 444\u201311 453.", "metadata": {"filetype": "application/pdf", "languages": ["eng"], "page_number": 9, "parent_id": "0200b826f862b4f75389fd989b474835", "filename": "rtx_paper.pdf"}}
{"type": "ListItem", "element_id": "4993d99adae177a0c388fd15b1d7d21c", "text": "[69] C. Eppner, A. Mousavian, and D. Fox, \u201cACRONYM: A large-scale grasp dataset based on simulation,\u201d in 2021 IEEE Int. Conf. on Robotics and Automation, ICRA, 2020.", "metadata": {"filetype": "application/pdf", "languages": ["eng"], "page_number": 9, "parent_id": "0200b826f862b4f75389fd989b474835", "filename": "rtx_paper.pdf"}}
{"type": "ListItem", "element_id": "5531a27cdc7a8961b3c64ced6f1c406c", "text": "[70] K. Bousmalis, A. Irpan, P. Wohlhart, Y. Bai, M. Kel- cey, M. Kalakrishnan, L. Downs, J. Ibarz, P. Pastor, K. Konolige, S. Levine, and V. Vanhoucke, \u201cUsing simulation and domain adaptation to improve effi- ciency of deep robotic grasping,\u201d in ICRA, 2018, pp. 4243\u20134250.", "metadata": {"filetype": "application/pdf", "languages": ["eng"], "page_number": 9, "parent_id": "0200b826f862b4f75389fd989b474835", "filename": "rtx_paper.pdf"}}
{"type": "ListItem", "element_id": "4ddca42d976659e85dd4da4bf60f9d28", "text": "[71] X. Zhu, R. Tian, C. Xu, M. Huo, W. Zhan, M. Tomizuka, and M. Ding, \u201cFanuc manipulation: A dataset for learning-based manipulation with fanuc mate 200iD robot,\u201d https://sites.google.com/berkeley. edu/fanuc-manipulation, 2023.", "metadata": {"filetype": "application/pdf", "languages": ["eng"], "page_number": 9, "parent_id": "0200b826f862b4f75389fd989b474835", "filename": "rtx_paper.pdf"}}
{"type": "NarrativeText", "element_id": "247b126c29426f5e3f5029ec102deb3d", "text": "[72] K.-T. Yu, M. Bauza, N. Fazeli, and A. Rodriguez, \u201cMore than a million ways to be pushed. a high- fidelity experimental dataset of planar pushing,\u201d in 2016 IEEE/RSJ international conference on intelligent robots and systems (IROS). IEEE, 2016, pp. 30\u201337. [73] C. Finn and S. Levine, \u201cDeep visual foresight for plan- ning robot motion,\u201d in 2017 IEEE International Con- ference on Robotics and Automation (ICRA). IEEE, 2017, pp. 2786\u20132793.", "metadata": {"filetype": "application/pdf", "languages": ["eng"], "page_number": 9, "parent_id": "0200b826f862b4f75389fd989b474835", "filename": "rtx_paper.pdf"}}
{"type": "ListItem", "element_id": "13ed4a16cde0a8a92d921a2108803c6a", "text": "[74] F. Ebert, C. Finn, S. Dasari, A. Xie, A. Lee, and S. Levine, \u201cVisual foresight: Model-based deep rein- forcement learning for vision-based robotic control,\u201d arXiv preprint arXiv:1812.00568, 2018.", "metadata": {"filetype": "application/pdf", "languages": ["eng"], "page_number": 9, "parent_id": "0200b826f862b4f75389fd989b474835", "filename": "rtx_paper.pdf"}}
{"type": "ListItem", "element_id": "d8ea6677c0e256ce8dd8be65bbc83b95", "text": "[75] P. Shilane, P. Min, M. Kazhdan, and T. Funkhouser, \u201cThe princeton shape benchmark,\u201d in Shape Modeling Applications, 2004, pp. 167\u2013388.", "metadata": {"filetype": "application/pdf", "languages": ["eng"], "page_number": 9, "parent_id": "0200b826f862b4f75389fd989b474835", "filename": "rtx_paper.pdf"}}
{"type": "ListItem", "element_id": "caecfec5623fc05c068c8cbca25ab97f", "text": "[76] W. Wohlkinger, A. Aldoma Buchaca, R. Rusu, and", "metadata": {"filetype": "application/pdf", "languages": ["eng"], "page_number": 9, "parent_id": "0200b826f862b4f75389fd989b474835", "filename": "rtx_paper.pdf"}}
{"type": "NarrativeText", "element_id": "99cadf9ad057b59f93bbbe90104513cf", "text": "M. Vincze, \u201c3DNet: Large-Scale Object Class Recog- nition from CAD Models,\u201d in IEEE International Con- ference on Robotics and Automation (ICRA), 2012.", "metadata": {"filetype": "application/pdf", "languages": ["eng"], "page_number": 10, "filename": "rtx_paper.pdf"}}
{"type": "ListItem", "element_id": "f2aacc78be20dd37461eae754e364905", "text": "[77] A. Kasper, Z. Xue, and R. Dillmann, \u201cThe kit object models database: An object model database for object recognition, localization and manipulation in service robotics,\u201d The International Journal of Robotics Re- search, vol. 31, no. 8, pp. 927\u2013934, 2012.", "metadata": {"filetype": "application/pdf", "languages": ["eng"], "page_number": 10, "filename": "rtx_paper.pdf"}}
{"type": "ListItem", "element_id": "2c98e4892851bede944275d837687249", "text": "[78] A. Singh, J. Sha, K. S. Narayan, T. Achim, and P. Abbeel, \u201cBigBIRD: A large-scale 3D database of object instances,\u201d in IEEE International Conference on Robotics and Automation (ICRA), 2014, pp. 509\u2013 516.", "metadata": {"filetype": "application/pdf", "languages": ["eng"], "page_number": 10, "filename": "rtx_paper.pdf"}}
{"type": "ListItem", "element_id": "0d09395ec24a4134b55e1b953e4e912e", "text": "[79] B. Calli, A. Walsman, A. Singh, S. Srinivasa, P. Abbeel, and A. M. Dollar, \u201cBenchmarking in ma- nipulation research: Using the Yale-CMU-Berkeley object and model set,\u201d IEEE Robotics & Automation Magazine, vol. 22, no. 3, pp. 36\u201352, 2015.", "metadata": {"filetype": "application/pdf", "languages": ["eng"], "page_number": 10, "filename": "rtx_paper.pdf"}}
{"type": "ListItem", "element_id": "71999dc6e522cc8647ba79222378c411", "text": "[80] Zhirong Wu, S. Song, A. Khosla, Fisher Yu, Linguang Zhang, Xiaoou Tang, and J. Xiao, \u201c3D ShapeNets: A deep representation for volumetric shapes,\u201d in IEEE Conference on Computer Vision and Pattern Recogni- tion (CVPR), 2015, pp. 1912\u20131920.", "metadata": {"filetype": "application/pdf", "languages": ["eng"], "page_number": 10, "filename": "rtx_paper.pdf"}}
{"type": "ListItem", "element_id": "bee5cb3d9c3cf81a443fb07e57637ad4", "text": "[81] Y. Xiang, W. Kim, W. Chen, J. Ji, C. Choy, H. Su, R. Mottaghi, L. Guibas, and S. Savarese, \u201cObject- Net3D: A large scale database for 3d object recog- nition,\u201d in European Conference on Computer Vision (ECCV). Springer, 2016, pp. 160\u2013176.", "metadata": {"filetype": "application/pdf", "languages": ["eng"], "page_number": 10, "filename": "rtx_paper.pdf"}}
{"type": "ListItem", "element_id": "1399c589e32af9a00d60849c560f66c1", "text": "[82] D. Morrison, P. Corke, and J. Leitner, \u201cEgad! an evolved grasping analysis dataset for diversity and re- producibility in robotic manipulation,\u201d IEEE Robotics and Automation Letters, vol. 5, no. 3, pp. 4368\u20134375, 2020.", "metadata": {"filetype": "application/pdf", "languages": ["eng"], "page_number": 10, "filename": "rtx_paper.pdf"}}
{"type": "ListItem", "element_id": "4e5dcaa3b51be9be1ec03286af4b0235", "text": "[83] R. Gao, Y.-Y. Chang, S. Mall, L. Fei-Fei, and J. Wu, \u201cObjectFolder: A dataset of objects with implicit vi- sual, auditory, and tactile representations,\u201d in Confer- ence on Robot Learning, 2021, pp. 466\u2013476.", "metadata": {"filetype": "application/pdf", "languages": ["eng"], "page_number": 10, "filename": "rtx_paper.pdf"}}
{"type": "ListItem", "element_id": "247087ac83a2f3bc62370feb6ad76940", "text": "[84] L. Downs, A. Francis, N. Koenig, B. Kinman, R. Hick- man, K. Reymann, T. B. McHugh, and V. Vanhoucke, \u201cGoogle scanned objects: A high-quality dataset of 3D scanned household items,\u201d in 2022 International Con- ference on Robotics and Automation (ICRA). IEEE, 2022, pp. 2553\u20132560.", "metadata": {"filetype": "application/pdf", "languages": ["eng"], "page_number": 10, "filename": "rtx_paper.pdf"}}
{"type": "ListItem", "element_id": "2fea2f512f3dcad3b36bd6ed3c7170c0", "text": "[85] D. Kalashnikov, J. Varley, Y. Chebotar, B. Swan- son, R. Jonschkowski, C. Finn, S. Levine, and K. Hausman, \u201cMT-Opt: Continuous multi-task robotic scale,\u201d arXiv preprint reinforcement arXiv:2104.08212, 2021.", "metadata": {"filetype": "application/pdf", "languages": ["eng"], "page_number": 10, "filename": "rtx_paper.pdf"}}
{"type": "ListItem", "element_id": "d0a360bd58dcdba0782261e1f233dd54", "text": "J. Booher, M. Spero, A. Tung, J. Gao, J. Emmons, A. Gupta, E. Orbay, S. Savarese, and L. Fei-Fei, \u201cRoboTurk: A crowdsourcing platform for robotic skill learning through imitation,\u201d CoRR, vol. abs/1811.02790, 2018. [Online]. Available: http://arxiv.org/abs/1811.02790", "metadata": {"filetype": "application/pdf", "languages": ["eng"], "page_number": 10, "filename": "rtx_paper.pdf"}}
{"type": "ListItem", "element_id": "77d38cc9baf0f1064e2165bdc8991ca3", "text": "[87] P. Sharma, L. Mohan, L. Pinto, and A. Gupta, \u201cMul- tiple interactions made easy (MIME): Large scale demonstrations data for imitation,\u201d in Conference on", "metadata": {"filetype": "application/pdf", "languages": ["eng"], "page_number": 10, "filename": "rtx_paper.pdf"}}
{"type": "NarrativeText", "element_id": "940fbf04550df8a78a460e2bc77ff00d", "text": "robot learning. PMLR, 2018, pp. 906\u2013915.", "metadata": {"filetype": "application/pdf", "languages": ["eng"], "page_number": 10, "filename": "rtx_paper.pdf"}}
{"type": "ListItem", "element_id": "c994cb4f69c4974171eb0f2b301d7812", "text": "J. Booher, M. Spero, A. Tung, A. Gupta, Y. Zhu, A. Garg, S. Savarese, and L. Fei- Fei, \u201cScaling robot supervision to hundreds of hours with RoboTurk: Robotic manipulation dataset through human reasoning and dexterity,\u201d in 2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS).", "metadata": {"filetype": "application/pdf", "languages": ["eng"], "page_number": 10, "filename": "rtx_paper.pdf"}}
{"type": "ListItem", "element_id": "1fc8125c536a81e62140d8fd456c22fc", "text": "[89] F. Ebert, Y. Yang, K. Schmeckpeper, B. Bucher, G. Georgakis, K. Daniilidis, C. Finn, and S. Levine, \u201cBridge data: Boosting generalization of robotic skills with cross-domain datasets,\u201d in Robotics: Science and Systems (RSS) XVIII, 2022.", "metadata": {"filetype": "application/pdf", "languages": ["eng"], "page_number": 10, "filename": "rtx_paper.pdf"}}
{"type": "ListItem", "element_id": "c58ca8e30e2b2eb45304ac7fd95be5d9", "text": "[90] A. Mandlekar, D. Xu, J. Wong, S. Nasiriany, C. Wang, R. Kulkarni, L. Fei-Fei, S. Savarese, Y. Zhu, and R. Mart\u00b4\u0131n-Mart\u00b4\u0131n, \u201cWhat matters in learning from offline human demonstrations for robot manipulation,\u201d in arXiv preprint arXiv:2108.03298, 2021.", "metadata": {"filetype": "application/pdf", "languages": ["eng"], "page_number": 10, "filename": "rtx_paper.pdf"}}
{"type": "ListItem", "element_id": "2b89a636de9202d9dde1aec5bbd2e22d", "text": "[91] C. Lynch, A. Wahid, J. Tompson, T. Ding, J. Betker, R. Baruch, T. Armstrong, and P. Florence, \u201cInterac- tive language: Talking to robots in real time,\u201d IEEE Robotics and Automation Letters, 2023.", "metadata": {"filetype": "application/pdf", "languages": ["eng"], "page_number": 10, "filename": "rtx_paper.pdf"}}
{"type": "NarrativeText", "element_id": "090bdb7239807baacc499dbb92072c36", "text": "[92] H.-S. Fang, H. Fang, Z. Tang, J. Liu, J. Wang, H. Zhu, and C. Lu, \u201cRH20T: A robotic dataset for learning diverse skills in one-shot,\u201d in RSS 2023 Workshop on Learning for Task and Motion Planning, 2023. [93] H. Bharadhwaj, J. Vakil, M. Sharma, A. Gupta, S. Tul- siani, and V. Kumar, \u201cRoboAgent: Towards sample efficient robot manipulation with semantic augmenta- tions and action chunking,\u201d arxiv, 2023.", "metadata": {"filetype": "application/pdf", "languages": ["eng"], "page_number": 10, "filename": "rtx_paper.pdf"}}
{"type": "ListItem", "element_id": "1b738539984f85368a21757ab751cbbb", "text": "[94] M. Heo, Y. Lee, D. Lee, and J. J. Lim, \u201cFurni- turebench: Reproducible real-world benchmark for long-horizon complex manipulation,\u201d in Robotics: Sci- ence and Systems, 2023.", "metadata": {"filetype": "application/pdf", "languages": ["eng"], "page_number": 10, "filename": "rtx_paper.pdf"}}
{"type": "ListItem", "element_id": "580ab9f24c23bbc0075f2c082269aefe", "text": "[95] H. Walke, K. Black, A. Lee, M. J. Kim, M. Du, C. Zheng, T. Zhao, P. Hansen-Estruch, Q. Vuong, A. He, V. Myers, K. Fang, C. Finn, and S. Levine, \u201cBridgedata v2: A dataset for robot learning at scale,\u201d 2023.", "metadata": {"filetype": "application/pdf", "languages": ["eng"], "page_number": 10, "filename": "rtx_paper.pdf"}}
{"type": "ListItem", "element_id": "e0a1380162b7a838c9e9e9d37f27f682", "text": "language,\u201d Cognitive Psychology, vol. 3, no. 1, pp. 1\u2013191, 1972. [Online]. Available: https://www.sciencedirect. com/science/article/pii/0010028572900023", "metadata": {"filetype": "application/pdf", "languages": ["eng"], "page_number": 10, "filename": "rtx_paper.pdf"}}
{"type": "ListItem", "element_id": "670d1fed000736e265422812ed06acac", "text": "[97] M. MacMahon, B. Stankiewicz, and B. Kuipers, \u201cWalk the talk: Connecting language, knowledge, and action in route instructions,\u201d in Proceedings of the Twenty-First AAAI Conference on Artificial Intelli- gence, 2006.", "metadata": {"filetype": "application/pdf", "languages": ["eng"], "page_number": 10, "filename": "rtx_paper.pdf"}}
{"type": "ListItem", "element_id": "36210f07cb38434fa5f36e8135c29fa4", "text": "[98] T. Kollar, S. Tellex, D. Roy, and N. Roy, \u201cToward understanding natural language directions,\u201d in 2010 5th ACM/IEEE International Conference on Human- Robot Interaction (HRI), 2010, pp. 259\u2013266.", "metadata": {"filetype": "application/pdf", "languages": ["eng"], "page_number": 10, "filename": "rtx_paper.pdf"}}
{"type": "ListItem", "element_id": "bd7d1047f6f4feb1dc19b7ca0ddbd3cb", "text": "[99] D. L. Chen and R. J. Mooney, \u201cLearning to interpret natural language navigation instructions from observa- tions,\u201d in Proceedings of the Twenty-Fifth AAAI Con- ference on Artificial Intelligence, 2011, p. 859\u2013865.", "metadata": {"filetype": "application/pdf", "languages": ["eng"], "page_number": 10, "filename": "rtx_paper.pdf"}}
{"type": "ListItem", "element_id": "79a5c2615ec073e2681a2935373c45f9", "text": "[100] F. Duvallet, J. Oh, A. Stentz, M. Walter, T. Howard,", "metadata": {"filetype": "application/pdf", "languages": ["eng"], "page_number": 10, "filename": "rtx_paper.pdf"}}
{"type": "NarrativeText", "element_id": "e84edde354eccc15217e99177483e079", "text": "S. Hemachandra, S. Teller, and N. Roy, \u201cInferring maps and behaviors from natural language instruc- tions,\u201d in International Symposium on Experimental Robotics (ISER), 2014.", "metadata": {"filetype": "application/pdf", "languages": ["eng"], "page_number": 11, "filename": "rtx_paper.pdf"}}
{"type": "NarrativeText", "element_id": "bb99f7d0fba7544d7c6fec24fa2f079f", "text": "[101] J. Luketina, N. Nardelli, G. Farquhar, J. N. Foer- ster, J. Andreas, E. Grefenstette, S. Whiteson, and T. Rockt\u00a8aschel, \u201cA survey of reinforcement learning informed by natural language,\u201d in IJCAI, 2019. [102] S. Stepputtis, J. Campbell, M. Phielipp, S. Lee, C. Baral, and H. Ben Amor, \u201cLanguage-conditioned imitation learning for robot manipulation tasks,\u201d Ad- vances in Neural Information Processing Systems, vol. 33, pp. 13 139\u201313 150, 2020.", "metadata": {"filetype": "application/pdf", "languages": ["eng"], "page_number": 11, "filename": "rtx_paper.pdf"}}
{"type": "ListItem", "element_id": "f40718194e964f4090435680eb6e4487", "text": "[103] S. Nair, E. Mitchell, K. Chen, S. Savarese, C. Finn et al., \u201cLearning language-conditioned robot behavior from offline data and crowd-sourced annotation,\u201d in Conference on Robot Learning. PMLR, 2022, pp. 1303\u20131315.", "metadata": {"filetype": "application/pdf", "languages": ["eng"], "page_number": 11, "filename": "rtx_paper.pdf"}}
{"type": "ListItem", "element_id": "a14ee028e66ca93d06dabac566658f01", "text": "and W. Burgard, \u201cCALVIN: A benchmark for language- conditioned policy learning for long-horizon robot manipulation tasks,\u201d IEEE Robotics and Automation Letters, 2022.", "metadata": {"filetype": "application/pdf", "languages": ["eng"], "page_number": 11, "filename": "rtx_paper.pdf"}}
{"type": "ListItem", "element_id": "01f36dd8c3117ed00b2713b2795e905b", "text": "[105] O. Mees, L. Hermann, and W. Burgard, \u201cWhat matters in language conditioned robotic imitation learning over unstructured data,\u201d IEEE Robotics and Automa- tion Letters, vol. 7, no. 4, pp. 11 205\u201311 212, 2022.", "metadata": {"filetype": "application/pdf", "languages": ["eng"], "page_number": 11, "filename": "rtx_paper.pdf"}}
{"type": "NarrativeText", "element_id": "735876fec68198d0e9cdcc101439ae55", "text": "[106] M. Shridhar, L. Manuelli, and D. Fox, \u201cPerceiver- actor: A multi-task transformer for robotic manipu- lation,\u201d Conference on Robot Learning (CoRL), 2022. [107] F. Hill, S. Mokra, N. Wong, and T. Harley, \u201cHuman instruction-following with deep reinforcement learn- ing via transfer-learning from text,\u201d arXiv preprint arXiv:2005.09382, 2020.", "metadata": {"filetype": "application/pdf", "languages": ["eng"], "page_number": 11, "filename": "rtx_paper.pdf"}}
{"type": "ListItem", "element_id": "d471bac574af2b4689978f10dd1878e1", "text": "[108] C. Lynch and P. Sermanet, \u201cGrounding language in play,\u201d Robotics: Science and Systems (RSS), 2021.", "metadata": {"filetype": "application/pdf", "languages": ["eng"], "page_number": 11, "filename": "rtx_paper.pdf"}}
{"type": "ListItem", "element_id": "3659c78131c4550133dfdc9441b92ec5", "text": "[109] M. Ahn, A. Brohan, N. Brown, Y. Chebotar, O. Cortes, B. David, C. Finn, K. Gopalakrishnan, K. Hausman, A. Herzog et al., \u201cDo as I can, not as I say: Grounding language in robotic affordances,\u201d Conference on Robot Learning (CoRL), 2022.", "metadata": {"filetype": "application/pdf", "languages": ["eng"], "page_number": 11, "filename": "rtx_paper.pdf"}}
{"type": "ListItem", "element_id": "fc5b8e9e14237a3559ed7676491a15d3", "text": "[110] Y. Jiang, A. Gupta, Z. Zhang, G. Wang, Y. Dou, Y. Chen, L. Fei-Fei, A. Anandkumar, Y. Zhu, and L. Fan, \u201cVIMA: General robot manipulation with multimodal prompts,\u201d International Conference on Machine Learning (ICML), 2023.", "metadata": {"filetype": "application/pdf", "languages": ["eng"], "page_number": 11, "filename": "rtx_paper.pdf"}}
{"type": "ListItem", "element_id": "312f9a3acc8a454e55ff37d7007c642e", "text": "[111] S. Vemprala, R. Bonatti, A. Bucker, and A. Kapoor, \u201cChatGPT for robotics: Design principles and model abilities,\u201d Microsoft Auton. Syst. Robot. Res, vol. 2, p. 20, 2023.", "metadata": {"filetype": "application/pdf", "languages": ["eng"], "page_number": 11, "filename": "rtx_paper.pdf"}}
{"type": "ListItem", "element_id": "6aa32d9a146053087ac093dd4d8dc91e", "text": "[112] W. Huang, C. Wang, R. Zhang, Y. Li, J. Wu, and L. Fei-Fei, \u201cVoxPoser: Composable 3d value maps for robotic manipulation with language models,\u201d arXiv preprint arXiv:2307.05973, 2023.", "metadata": {"filetype": "application/pdf", "languages": ["eng"], "page_number": 11, "filename": "rtx_paper.pdf"}}
{"type": "ListItem", "element_id": "adbe4c614a5847f0587a0c9719bab8a5", "text": "[113] M. Shridhar, L. Manuelli, and D. Fox, \u201cCliport: What and where pathways for robotic manipulation,\u201d in Conference on Robot Learning. PMLR, 2022, pp.", "metadata": {"filetype": "application/pdf", "languages": ["eng"], "page_number": 11, "filename": "rtx_paper.pdf"}}
{"type": "NarrativeText", "element_id": "05477b16bbe0e1c385923e30a04d62ec", "text": "894\u2013906.", "metadata": {"filetype": "application/pdf", "languages": ["eng"], "page_number": 11, "filename": "rtx_paper.pdf"}}
{"type": "ListItem", "element_id": "c0fba499b8f0d71acaf4bc2570954ca2", "text": "[114] A. Stone, T. Xiao, Y. Lu, K. Gopalakrishnan, K.-H. Lee, Q. Vuong, P. Wohlhart, B. Zitkovich, F. Xia, C. Finn et al., \u201cOpen-world object manipulation using pre-trained vision-language models,\u201d arXiv preprint arXiv:2303.00905, 2023.", "metadata": {"filetype": "application/pdf", "languages": ["eng"], "page_number": 11, "filename": "rtx_paper.pdf"}}
{"type": "ListItem", "element_id": "ba3a7b8252a119ff2b6c8e4eed6a8400", "text": "[115] Y. Mu, Q. Zhang, M. Hu, W. Wang, M. Ding, J. Jin, B. Wang, J. Dai, Y. Qiao, and P. Luo, \u201cEmbodiedGPT: Vision-language pre-training via embodied chain of thought,\u201d arXiv preprint arXiv:2305.15021, 2023.", "metadata": {"filetype": "application/pdf", "languages": ["eng"], "page_number": 11, "filename": "rtx_paper.pdf"}}
{"type": "ListItem", "element_id": "21fb6dd56f5847e8d45f5c5c6d5cc3bc", "text": "[116] E. Perez, F. Strub, H. de Vries, V. Dumoulin, and A. Courville, \u201cFilm: Visual reasoning with a general conditioning layer,\u201d 2017.", "metadata": {"filetype": "application/pdf", "languages": ["eng"], "page_number": 11, "filename": "rtx_paper.pdf"}}
{"type": "ListItem", "element_id": "9f44c8ab2baf109ef64bc09e89e3d59e", "text": "[117] M. Tan and Q. Le, \u201cEfficientNet: Rethinking model scaling for convolutional neural networks,\u201d in Inter- national conference on machine learning. PMLR, 2019, pp. 6105\u20136114.", "metadata": {"filetype": "application/pdf", "languages": ["eng"], "page_number": 11, "filename": "rtx_paper.pdf"}}
{"type": "ListItem", "element_id": "655c345e1290120200c3620f2921fe8d", "text": "[118] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, \u0141. Kaiser, and I. Polosukhin, \u201cAttention is all you need,\u201d Advances in neural infor- mation processing systems, vol. 30, 2017.", "metadata": {"filetype": "application/pdf", "languages": ["eng"], "page_number": 11, "filename": "rtx_paper.pdf"}}
{"type": "ListItem", "element_id": "5fcfebf207c74d2a523e22f55ce64fc0", "text": "[119] S. Ramos, S. Girgin, L. Hussenot, D. Vincent, H. Yakubovich, D. Toyama, A. Gergely, P. Stanczyk, R. Marinier, J. Harmsen, O. Pietquin, and N. Mom- chev, \u201cRLDS: an ecosystem to generate, share and use datasets in reinforcement learning,\u201d 2021.", "metadata": {"filetype": "application/pdf", "languages": ["eng"], "page_number": 11, "filename": "rtx_paper.pdf"}}
{"type": "ListItem", "element_id": "d086a6bf316e9ed68c7961e06a2205b5", "text": "[120] D. Cer, Y. Yang, S. yi Kong, N. Hua, N. Limti- aco, R. S. John, N. Constant, M. Guajardo-Cespedes, S. Yuan, C. Tar, Y.-H. Sung, B. Strope, and R. Kurzweil, \u201cUniversal sentence encoder,\u201d 2018.", "metadata": {"filetype": "application/pdf", "languages": ["eng"], "page_number": 11, "filename": "rtx_paper.pdf"}}
{"type": "NarrativeText", "element_id": "0849fc423df031fe1d508bd87cd7cf29", "text": "[121] X. Chen, J. Djolonga, P. Padlewski, B. Mustafa, S. Changpinyo, J. Wu, C. R. Ruiz, S. Goodman, X. Wang, Y. Tay, S. Shakeri, M. Dehghani, D. Salz, M. Lucic, M. Tschannen, A. Nagrani, H. Hu, M. Joshi, B. Pang, C. Montgomery, P. Pietrzyk, M. Ritter, A. Piergiovanni, M. Minderer, F. Pavetic, A. Waters, G. Li, I. Alabdulmohsin, L. Beyer, J. Amelot, K. Lee, A. P. Steiner, Y. Li, D. Keysers, A. Arnab, Y. Xu, K. Rong, A. Kolesnikov, M. Seyedhosseini, A. An- gelova, X. Zhai, N. Houlsby, and R. Soricut, \u201cPali- x: On scaling up a multilingual vision and language model,\u201d 2023.", "metadata": {"filetype": "application/pdf", "languages": ["eng"], "page_number": 11, "filename": "rtx_paper.pdf"}}
{"type": "NarrativeText", "element_id": "e345b83be3dee0bece86d562e76dd50a", "text": "[122] J.-B. Alayrac, J. Donahue, P. Luc, A. Miech, I. Barr, Y. Hasson, K. Lenc, A. Mensch, K. Millican, M. Reynolds, R. Ring, E. Rutherford, S. Cabi, T. Han, Z. Gong, S. Samangooei, M. Monteiro, J. Menick, S. Borgeaud, A. Brock, A. Nematzadeh, S. Shar- ifzadeh, M. Binkowski, R. Barreira, O. Vinyals, A. Zisserman, and K. Simonyan, \u201cFlamingo: a visual language model for few-shot learning,\u201d 2022. [123] D. Driess, F. Xia, M. S. M. Sajjadi, C. Lynch, A. Chowdhery, B. Ichter, A. Wahid, J. Tompson, Q. Vuong, T. Yu, W. Huang, Y. Chebotar, P. Sermanet, D. Duckworth, S. Levine, V. Vanhoucke, K. Hausman, M. Toussaint, K. Greff, A. Zeng, I. Mordatch, and P. Florence, \u201cPaLM-E: An embodied multimodal lan- guage model,\u201d 2023.", "metadata": {"filetype": "application/pdf", "languages": ["eng"], "page_number": 11, "filename": "rtx_paper.pdf"}}
{"type": "ListItem", "element_id": "467e050aa66504cbd2439cec039e08ca", "text": "[124] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weis- senborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, J. Uszkoreit, and N. Houlsby, \u201cAn image is worth 16x16 words: Transformers for image recognition at scale,\u201d 2021.", "metadata": {"filetype": "application/pdf", "languages": ["eng"], "page_number": 12, "filename": "rtx_paper.pdf"}}
{"type": "NarrativeText", "element_id": "52f764e46975617975fe3235c5e2cab8", "text": "[125] Y. Tay, M. Dehghani, V. Q. Tran, X. Garcia, J. Wei, X. Wang, H. W. Chung, S. Shakeri, D. Bahri, T. Schus- ter, H. S. Zheng, D. Zhou, N. Houlsby, and D. Metzler, \u201cUL2: Unifying language learning paradigms,\u201d 2023. [126] E. Rosete-Beas, O. Mees, G. Kalweit, J. Boedecker, and W. Burgard, \u201cLatent plans for task agnostic offline the 6th reinforcement Conference on Robot Learning (CoRL), 2022. [127] O. Mees, J. Borja-Diaz, and W. Burgard, \u201cGrounding language with visual affordances over unstructured data,\u201d in Proceedings of the IEEE International Con- ference on Robotics and Automation (ICRA), London, UK, 2023.", "metadata": {"filetype": "application/pdf", "languages": ["eng"], "page_number": 12, "filename": "rtx_paper.pdf"}}
{"type": "ListItem", "element_id": "b2da1e3a04aa122f2c06afb19c384d9d", "text": "[128] S. Dass, J. Yapeter, J. Zhang, J. Zhang, K. Pertsch, S. Nikolaidis, and J. J. Lim, \u201cCLVR jaco play dataset,\u201d 2023. [Online]. Available: https://github. com/clvrai/clvr jaco play dataset", "metadata": {"filetype": "application/pdf", "languages": ["eng"], "page_number": 12, "filename": "rtx_paper.pdf"}}
{"type": "ListItem", "element_id": "5f1ac884b538a53f7f9b7d90ee244914", "text": "[129] J. Luo, C. Xu, X. Geng, G. Feng, K. Fang, L. Tan, S. Schaal, and S. Levine, \u201cMulti-stage cable rout- imitation learning,\u201d arXiv ing through hierarchical preprint arXiv:2307.08927, 2023.", "metadata": {"filetype": "application/pdf", "languages": ["eng"], "page_number": 12, "filename": "rtx_paper.pdf"}}
{"type": "ListItem", "element_id": "5a22a82919ab26280b0e675b7877ac4b", "text": "[130] J. Pari, N. M. Shafiullah, S. P. Arunachalam, and L. Pinto, \u201cThe surprising effectiveness of representa- tion learning for visual imitation,\u201d 2021.", "metadata": {"filetype": "application/pdf", "languages": ["eng"], "page_number": 12, "filename": "rtx_paper.pdf"}}
{"type": "ListItem", "element_id": "592a4f2a1b9db0aa9c95dd5d60ae4e57", "text": "[131] Y. Zhu, A. Joshi, P. Stone, and Y. Zhu, \u201cViola: Imitation learning for vision-based manipulation with object proposal priors,\u201d 2023.", "metadata": {"filetype": "application/pdf", "languages": ["eng"], "page_number": 12, "filename": "rtx_paper.pdf"}}
{"type": "ListItem", "element_id": "42c12fcc5d4766a558dbafd8595daeee", "text": "[132] L. Y. Chen, S. Adebola, and K. Goldberg, \u201cBerkeley UR5 demonstration dataset,\u201d https://sites.google.com/ view/berkeley-ur5/home.", "metadata": {"filetype": "application/pdf", "languages": ["eng"], "page_number": 12, "filename": "rtx_paper.pdf"}}
{"type": "ListItem", "element_id": "f35b7e87751e71865ade25b85456edb4", "text": "[133] G. Zhou, V. Dean, M. K. Srirama, A. Rajeswaran, J. Pari, K. Hatch, A. Jain, T. Yu, P. Abbeel, L. Pinto, C. Finn, and A. Gupta, \u201cTrain offline, test online: A real robot learning benchmark,\u201d 2023.", "metadata": {"filetype": "application/pdf", "languages": ["eng"], "page_number": 12, "filename": "rtx_paper.pdf"}}
{"type": "ListItem", "element_id": "8a1b254776f981ea5d1cfa4fdc6493fc", "text": "[134] \u201cTask-agnostic real world robot play,\u201d https://www. kaggle.com/datasets/oiermees/taco-robot.", "metadata": {"filetype": "application/pdf", "languages": ["eng"], "page_number": 12, "filename": "rtx_paper.pdf"}}
