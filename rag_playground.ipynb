{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a RAG Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing with Unstructured.io\n",
    "The first leg of the RAG pipeline involves taking unstructured data from myriad formats and converting it to semi-structured records (JSON). Unstructured.io provides the toolkit that makes it easy to build a pipeline that transforms many forms of unstructured data. In this demo, I focus on PDF extraction because it's one of the most difficult documents to parse. However, unstructured.io provides the tools to parse many document types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "# Set up logging to display only ERROR and higher level messages\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "\n",
    "from utils import Utils, Preprocess\n",
    "from unstructured_client import UnstructuredClient\n",
    "\n",
    "utils = Utils()\n",
    "\n",
    "UNSTRUCTURED_API_KEY = utils.get_api_key(\"UNSTRUCTURED\")\n",
    "client = UnstructuredClient(api_key_auth=UNSTRUCTURED_API_KEY)\n",
    "\n",
    "filename = \"rtx_paper.pdf\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Documents like HTML, Word, and Markdown can be processed with rules-based parsers because they include formatting information inherenet in the document structure (ie. HTML tags for headers, tables, etc). However, other documents like PDFs and images don't have this structural metadata, so it has to be inferred visually. **Yolox is the Document Layout Detection model I used to draw bounding boxes around text for classification.**   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# instantiate Preprocess class with file path\n",
    "doc = Preprocess(filename)\n",
    "\n",
    "# read in the file\n",
    "files = doc.read_file()\n",
    "\n",
    "# build request (instruct the API on how to parse the file)\n",
    "req = doc.partition_file(files, strategy='hi_res', model_name='yolox')\n",
    "\n",
    "# store the parsed file as records (list of dictionaries) and Elements (list of Element objects)\n",
    "records, elements = doc.get_structured_text(client, req)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An **Element** is the atomic object of the unstructured.io output. It represents a component of the source document that has been partitioned and is designed to preserve the semantic structure of the document. Each element object has the following information:\n",
    "- **Type:** indicates the type of element (NarrativeText, Title, Header, etc)\n",
    "- **Element ID:** unique identifier for the element\n",
    "- **Text:** the extracted text content of the element\n",
    "- **Metadata:** dictionary containing additional information about the element\n",
    "\n",
    "Sometimes it's easier to inspect the partitioned output as records. Other times, it's easier with the Element object. It's the same content either way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspecting the partitioned output is important to get a feel for what matters and what doesn't. In this case, I'm building a RAG pipeline for arXiv research papers. These PDFs tend to be similarly structured (with headers, references, tables and images)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ListItem', 101),\n",
       " ('NarrativeText', 64),\n",
       " ('Title', 6),\n",
       " ('Image', 5),\n",
       " ('FigureCaption', 5),\n",
       " ('UncategorizedText', 4),\n",
       " ('Table', 2)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils import Inspect\n",
    "\n",
    "# instantiate Inspect class\n",
    "inspect = Inspect(records=records, elements=elements)\n",
    "\n",
    "# get unique elements and counts\n",
    "inspect.count_elements()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, I can see there are seven types of elements in the PDF. For example, there are 101 elements classified as ListItem (this seems like a lot). I can now start to understand what elements are important (ie. not only what I care about in this particular PDF, but what will generalize over all arXiv PDFs I want to query in the future).\n",
    "\n",
    "To start, there's six title elements. When I inspect these, I see they're section headers. These will be useful (more on this later). However, when I look at the document , it's clear that not all section headers were identified. There's 15 section headers in the document (I counted these manually), so only about a third were identified. There's also an erroneous title (the first one).\n",
    "\n",
    "**There's room for improvement in the documment layout detection model. It would perhaps be worth having a separate model just for Title identification as this is the most crucial category to identify**\n",
    "\n",
    "While this isn't perfect, the Titles are still good to have (I'll use them later). Removing the erroneus title won't generalize to other arXiv papers, so not worth discarding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'735c6af3e3d8a967022eda94d9e1434f': 'O R . s c [',\n",
       " '7e8a689d89b024a7dc47ebee59bb07fa': 'III. THE OPEN X-EMBODIMENT REPOSITORY',\n",
       " '482139f50b98da4165176eea35acb871': 'B. Dataset Analysis',\n",
       " 'e110774179bc4d92f2382f2063c16a2f': 'IV. RT-X DESIGN',\n",
       " '6ffcbfd5e933ece63fe9d715a734707c': 'A. Data format consolidation',\n",
       " '0200b826f862b4f75389fd989b474835': 'REFERENCES'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get dictionary of all Title elements with unique IDs\n",
    "section_ids = inspect.get_section_id_dict()\n",
    "section_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Document Layout Detection works by first using computer vision models (I used YOLOX) to classify text as one of the various elements it's trained to identify and put bounding boxes around those elements. Then text is extracted from the bounding box using Object Character Recognition (OCR) if text can't be extracted directly from the document without it.\n",
    "\n",
    "I wondered whether the issue was with the bounding box or the text extraction, so I wrote the following check. \"II. Related Work\" is one of the section headers in the PDF. If it were missing, it would be a text extraction issue. But it's there, just nested in a NarrativeText element.\n",
    "\n",
    "Takeaway: the bounding box isn't super accurate, but this isn't a major issue (failed text extraction would have been). As an alternative approach, I could try using a Vision Transformer, where the image/PDF is encoded and text is output in one step, but this is subject to hallucination. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'element_id': 'af2c605feab1658a4551124324242f92',\n",
      " 'metadata': {'filename': 'rtx_paper.pdf',\n",
      "              'filetype': 'application/pdf',\n",
      "              'languages': ['eng'],\n",
      "              'page_number': 2},\n",
      " 'text': 'Following this rationale, we have two goals: (1) Evaluate whether '\n",
      "         'policies trained on data from many different robots and environments '\n",
      "         'enjoy the benefits of positive transfer, attaining better '\n",
      "         'performance than policies trained only on data from each evaluation '\n",
      "         'setup. (2) Organize large robotic datasets to enable future research '\n",
      "         'on X-embodiment models. We focus our work on robotic manipulation. '\n",
      "         'Addressing goal (1), our empirical contribution is to demonstrate '\n",
      "         'that several recent robotic learning methods, with minimal mod- '\n",
      "         'ification, can utilize X-embodiment data and enable positive '\n",
      "         'transfer. Specifically, we train the RT-1 [8] and RT-2 [9] models on '\n",
      "         '9 different robotic manipulators. We show that the resulting models, '\n",
      "         'which we call RT-X, can improve over policies trained only on data '\n",
      "         'from the evaluation domain, exhibiting better generalization and new '\n",
      "         'capabilities. Ad- dressing (2), we provide the Open X-Embodiment '\n",
      "         '(OXE) Repository, which includes a dataset with 22 different robotic '\n",
      "         'embodiments from 21 different institutions that can enable the '\n",
      "         'robotics community to pursue further research on X- embodiment '\n",
      "         'models, along with open-source tools to facili- tate such research. '\n",
      "         'Our aim is not to innovate in terms of the particular architectures '\n",
      "         'and algorithms, but rather to provide the model that we trained '\n",
      "         'together with data and tools to energize research around '\n",
      "         'X-embodiment robotic learning. II. RELATED WORK Transfer across '\n",
      "         'embodiments. A number of prior works have studied methods for '\n",
      "         'transfer across robot embodiments in simulation [10–22] and on real '\n",
      "         'robots [23–29]. These methods often introduce mechanisms '\n",
      "         'specifically designed to address the embodiment gap between '\n",
      "         'different robots, such as shared action representations [14, 30], '\n",
      "         'incorporating rep- resentation learning objectives [17, 26], '\n",
      "         'adapting the learned policy on embodiment information [11, 15, 18, '\n",
      "         '30, 31], and decoupling robot and environment representations [24]. '\n",
      "         'Prior work has provided initial demonstrations of X-embodiment '\n",
      "         'training [27] and transfer [25, 29, 32] with transformer models. We '\n",
      "         'investigate complementary architectures and provide complementary '\n",
      "         'analyses, and, in particular, study the interaction between '\n",
      "         'X-embodiment transfer and web-scale pretraining. Similarly, methods '\n",
      "         'for transfer across human and robot embodiments also often employ '\n",
      "         'techniques for reducing the embodiment gap, i.e. by translating '\n",
      "         'between domains or learning transferable representations [33–43]. '\n",
      "         'Al- ternatively, some works focus on sub-aspects of the problem such '\n",
      "         'as learning transferable reward functions [17, 44–48], goals [49, '\n",
      "         '50], dynamics models [51], or visual representa- tions [52–59] from '\n",
      "         'human video data. Unlike most of these prior works, we directly '\n",
      "         'train a policy on X-embodiment data, without any mechanisms to '\n",
      "         'reduce the embodiment gap, and observe positive transfer by '\n",
      "         'leveraging that data. Large-scale robot learning datasets. The robot '\n",
      "         'learning community has created open-source robot learning datasets,',\n",
      " 'type': 'NarrativeText'}\n"
     ]
    }
   ],
   "source": [
    "# only 5 of 15 section titles are accurately identified in the metadata\n",
    "# the unidentified section titles show up in narrative text not assigned a parent id \n",
    "from pprint import pprint\n",
    "\n",
    "lookup_string = 'II. RELATED WORK'\n",
    "\n",
    "for record in records:\n",
    "    if lookup_string in record['text']:\n",
    "        pprint(record)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the record types to see if the content is useful.\n",
    "- ListItem: these are important bulllet points that should be stored\n",
    "- Image: text from the images isn't well structured, so it's not very interpretable\n",
    "- UncategorizedText: not much useful information, mainly list of contributor names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'type': 'ListItem',\n",
       "  'element_id': '6821f000be4d925646adb547846faa3a',\n",
       "  'text': '• Open X-Embodiment Dataset: robot learning dataset with 1M+ robot trajectories from 22 robot embodi- ments.',\n",
       "  'metadata': {'filetype': 'application/pdf',\n",
       "   'languages': ['eng'],\n",
       "   'page_number': 3,\n",
       "   'parent_id': '7e8a689d89b024a7dc47ebee59bb07fa',\n",
       "   'filename': 'rtx_paper.pdf'}},\n",
       " {'type': 'ListItem',\n",
       "  'element_id': '784f76bdee32da2b616660d706a6013b',\n",
       "  'text': '• Pre-Trained Checkpoints: a selection of RT-X model checkpoints ready for inference and finetuning.',\n",
       "  'metadata': {'filetype': 'application/pdf',\n",
       "   'languages': ['eng'],\n",
       "   'page_number': 3,\n",
       "   'parent_id': '7e8a689d89b024a7dc47ebee59bb07fa',\n",
       "   'filename': 'rtx_paper.pdf'}}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect two records of type 'ListItem'\n",
    "# --> 'text' key shows these are informative bullet points that should be stored\n",
    "inspect.inspect_record_type('ListItem', max_items=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'type': 'Image',\n",
       "  'element_id': '89eef53e1c83117154f60e8d8bfaaaab',\n",
       "  'text': 'TOTO from Cable Routing RT-1 QT-Opt 1 @ 1M Episodes 311 Scenes [ ] — — L Research Labs across 21 Institutions — pour pick anything, 22 Embodiments ‘sweep the green Cloth to the lft ¢ side of the table RERE & RS pick green chip bag from counter S ) 527 Skills L A i o — A A 5 e e A o A= : set the bowl to o1 T8 b L pour stack route the right side of the table sl Stach cups 60 Datasets & place the black block L‘. & ﬁbo wl i the dish rack Y /E. 1) Pog < = A A 1,798 Attributes. 5,228 Objects. 23,486 S elations i = Jaco Play ALOHA Bridge Opening',\n",
       "  'metadata': {'filetype': 'application/pdf',\n",
       "   'languages': ['eng'],\n",
       "   'page_number': 1,\n",
       "   'filename': 'rtx_paper.pdf'}},\n",
       " {'type': 'Image',\n",
       "  'element_id': '7eb04e8cb6b49bb456577b8e9adea2df',\n",
       "  'text': 'In ;% & FLS £ Sawyer < & &5 S § S 5 $ ;“ & § e’p & $ K 4 Kinova Gen3 FES & ° & K4 ey g S € ¢ Hello Stretch Bﬁcs\\'l g e i WidowX Jackal WidowX Sawyer ¥ (a) # Datasets per Robot Embodiment (b) # Scenes per Embodiment (c) # Trajectories per Embodiment 14000 Shapes el 12000 150000 10000 Containers 125000 5000 100000 ppliances 75000 5000 0000 2000 Utensils 25000 2000 o ] £ I Lo & S5 59 ¢ OF, SIS S oo 3 § S §&8 & & $ & $ ;ﬁg f@ & & & & SSES & 8 .4 & £ 5 i Food 3;: SEES & £ \"JY a & @f (d) Common Dataset Skills (e) Common Dataset Objects',\n",
       "  'metadata': {'filetype': 'application/pdf',\n",
       "   'languages': ['eng'],\n",
       "   'page_number': 3,\n",
       "   'filename': 'rtx_paper.pdf'}}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# image items don't contain much useful text information\n",
    "# --> remove image content from data set\n",
    "inspect.inspect_record_type('Image', 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'type': 'UncategorizedText',\n",
       "  'element_id': 'f50ad2eb90733ac345a808a1a9e6e75f',\n",
       "  'text': '4 2 0 2',\n",
       "  'metadata': {'filetype': 'application/pdf',\n",
       "   'languages': ['eng'],\n",
       "   'page_number': 1,\n",
       "   'filename': 'rtx_paper.pdf'}},\n",
       " {'type': 'UncategorizedText',\n",
       "  'element_id': '7630b82f56224e713057fe23dbfc1122',\n",
       "  'text': ']',\n",
       "  'metadata': {'filetype': 'application/pdf',\n",
       "   'languages': ['eng'],\n",
       "   'page_number': 1,\n",
       "   'filename': 'rtx_paper.pdf'}},\n",
       " {'type': 'UncategorizedText',\n",
       "  'element_id': '661298c7e2144c15579c599385497ab9',\n",
       "  'text': '6 v 4 6 8 8 0 . 0 1 3 2 : v i X r a',\n",
       "  'metadata': {'filetype': 'application/pdf',\n",
       "   'languages': ['eng'],\n",
       "   'page_number': 1,\n",
       "   'parent_id': '735c6af3e3d8a967022eda94d9e1434f',\n",
       "   'filename': 'rtx_paper.pdf'}},\n",
       " {'type': 'UncategorizedText',\n",
       "  'element_id': 'bd57b596576ece5d43181c3f0e568935',\n",
       "  'text': 'Open X-Embodiment: Robotic Learning Datasets and RT-X Models Open X-Embodiment Collaboration0 robotics-transformer-x.github.io Abby O’Neill32, Abdul Rehman35, Abhiram Maddukuri43, Abhishek Gupta44, Abhishek Padalkar10, Abraham Lee32, Acorn Pooley11, Agrim Gupta27, Ajay Mandlekar21, Ajinkya Jain15, Albert Tung27, Alex Bewley11, Alex Herzog11, Alex Irpan11, Alexander Khazatsky27, Anant Rai22, Anchit Gupta19, Andrew Wang32, Anikait Singh11,32, Animesh Garg9, Aniruddha Kembhavi1, Annie Xie27, Anthony Brohan11, Antonin Raffin10, Archit Sharma27, Arefeh Yavary33, Arhan Jain44, Ashwin Balakrishna31, Ayzaan Wahid11, Ben Burgess-Limerick24, Beomjoon Kim17, Bernhard Sch¨olkopf18, Blake Wulfe31, Brian Ichter11, Cewu Lu26,8, Charles Xu32, Charlotte Le32, Chelsea Finn11,27, Chen Wang27, Chenfeng Xu32, Cheng Chi5,27, Chenguang Huang36, Christine Chan11, Christopher Agia27, Chuer Pan27, Chuyuan Fu11, Coline Devin11, Danfei Xu9, Daniel Morton27, Danny Driess11, Daphne Chen44, Deepak Pathak4, Dhruv Shah32, Dieter B¨uchler18, Dinesh Jayaraman40, Dmitry Kalashnikov11, Dorsa Sadigh11, Edward Johns14, Ethan Foster27, Fangchen Liu32, Federico Ceola16, Fei Xia11, Feiyu Zhao13, Freek Stulp10, Gaoyue Zhou22, Gaurav S. Sukhatme41, Gautam Salhotra41,15, Ge Yan34, Gilbert Feng32, Giulio Schiavi7, Glen Berseth39,20, Gregory Kahn32, Guanzhi Wang3,21, Hao Su34, Hao-Shu Fang26, Haochen Shi27, Henghui Bao41, Heni Ben Amor2, Henrik I Christensen34, Hiroki Furuta30, Homer Walke32, Hongjie Fang26, Huy Ha5,27, Igor Mordatch11, Ilija Radosavovic32, Isabel Leal11, Jacky Liang11, Jad Abou-Chakra24, Jaehyung Kim17, Jaimyn Drake32, Jan Peters28, Jan Schneider18, Jasmine Hsu11, Jeannette Bohg27, Jeffrey Bingham11, Jeffrey Wu32, Jensen Gao27, Jiaheng Hu29, Jiajun Wu27, Jialin Wu12, Jiankai Sun27, Jianlan Luo32, Jiayuan Gu34, Jie Tan11, Jihoon Oh30, Jimmy Wu23, Jingpei Lu34, Jingyun Yang27, Jitendra Malik32, Jo˜ao Silv´erio10, Joey Hejna27, Jonathan Booher27, Jonathan Tompson11, Jonathan Yang27, Jordi Salvador1, Joseph J. Lim17, Junhyek Han17, Kaiyuan Wang34, Kanishka Rao11, Karl Pertsch32,27, Karol Hausman11, Keegan Go15, Keerthana Gopalakrishnan11, Ken Goldberg32, Kendra Byrne11, Kenneth Oslund11, Kento Kawaharazuka30, Kevin Black32, Kevin Lin27, Kevin Zhang4, Kiana Ehsani1, Kiran Lekkala41, Kirsty Ellis39, Krishan Rana24, Krishnan Srinivasan27, Kuan Fang32, Kunal Pratap Singh6, Kuo-Hao Zeng1, Kyle Hatch31, Kyle Hsu27, Laurent Itti41, Lawrence Yunliang Chen32, Lerrel Pinto22, Li Fei-Fei27, Liam Tan32, Linxi ”Jim” Fan21, Lionel Ott7, Lisa Lee11, Luca Weihs1, Magnum Chen13, Marion Lepert27, Marius Memmel44, Masayoshi Tomizuka32, Masha Itkina31, Mateo Guaman Castro44, Max Spero27, Maximilian Du27, Michael Ahn11, Michael C. Yip34, Mingtong Zhang37, Mingyu Ding32, Minho Heo17, Mohan Kumar Srirama4, Mohit Sharma4, Moo Jin Kim27, Naoaki Kanazawa30, Nicklas Hansen34, Nicolas Heess11, Nikhil J Joshi11, Niko Suenderhauf24, Ning Liu13, Norman Di Palo14, Nur Muhammad Mahi Shafiullah22, Oier Mees36, Oliver Kroemer4, Osbert Bastani40, Pannag R Sanketi11, Patrick ”Tree” Miller31, Patrick Yin44, Paul Wohlhart11, Peng Xu11, Peter David Fagan35, Peter Mitrano38, Pierre Sermanet11, Pieter Abbeel32, Priya Sundaresan27, Qiuyu Chen44, Quan Vuong11, Rafael Rafailov11,27, Ran Tian32, Ria Doshi32, Roberto Mart´ın-Mart´ın29, Rohan Baijal44, Rosario Scalise44, Rose Hendrix1, Roy Lin32, Runjia Qian13, Ruohan Zhang27, Russell Mendonca4, Rutav Shah29, Ryan Hoque32, Ryan Julian11, Samuel Bustamante10, Sean Kirmani11, Sergey Levine11,32, Shan Lin34, Sherry Moore11, Shikhar Bahl4, Shivin Dass41,29, Shubham Sonawani2, Shuran Song5, Sichun Xu11, Siddhant Haldar22, Siddharth Karamcheti27, Simeon Adebola32, Simon Guist18, Soroush Nasiriany29, Stefan Schaal15, Stefan Welker11, Stephen Tian27, Subramanian Ramamoorthy35, Sudeep Dasari4, Suneel Belkhale27, Sungjae Park17, Suraj Nair31, Suvir Mirchandani27, Takayuki Osa30, Tanmay Gupta1, Tatsuya Harada30,25, Tatsuya Matsushima30, Ted Xiao11, Thomas Kollar31, Tianhe Yu11, Tianli Ding11, Todor Davchev11, Tony Z. Zhao27, Travis Armstrong11, Trevor Darrell32, Trinity Chung32, Vidhi Jain11,4, Vincent Vanhoucke11, Wei Zhan32, Wenxuan Zhou11,4, Wolfram Burgard42, Xi Chen11, Xiaolong Wang34, Xinghao Zhu32, Xinyang Geng32, Xiyuan Liu13, Xu Liangwei13, Xuanlin Li34, Yao Lu11, Yecheng Jason Ma40, Yejin Kim1, Yevgen Chebotar11, Yifan Zhou2, Yifeng Zhu29, Yilin Wu4, Ying Xu11, Yixuan Wang37, Yonatan Bisk4, Yoonyoung Cho17, Youngwoon Lee32, Yuchen Cui27, Yue Cao13, Yueh-Hua Wu34, Yujin Tang11,30, Yuke Zhu29, Yunchu Zhang44, Yunfan Jiang27, Yunshuang Li40, Yunzhu Li37, Yusuke Iwasawa30, Yutaka Matsuo30, Zehan Ma32, Zhuo Xu11, Zichen Jeff Cui22, Zichen Zhang1, Zipeng Fu27, Zipeng Lin32',\n",
       "  'metadata': {'filetype': 'application/pdf',\n",
       "   'languages': ['eng'],\n",
       "   'page_number': 1,\n",
       "   'parent_id': '735c6af3e3d8a967022eda94d9e1434f',\n",
       "   'filename': 'rtx_paper.pdf'}}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# uncategorized text doesn't contain much useful text information (it's a long list of contributor names)\n",
    "# --> remove uncategoriged text content from data set\n",
    "inspect.inspect_record_type('uncategorizedtext')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I wrote a method to view an org chart of the data. It shows every parent element along with the type and number of child elements that belong to it. I see that half of the UncategorizedText belongs to the erroneous Title section and the other half is without a parent. I can be comfortable dropping these.\n",
    "\n",
    "Generally, this method helps build intuition about the data. For example, the title 'III. The Open X-embodimment Repository' has two child element types that belong to it (NarrativeText and ListItem). But there were a bunch of list items (101)... I can see that many of them belong to the References section (which makes sense). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PARENT TYPE: Title, PARENT ID: 735c6af3e3d8a967022eda94d9e1434f\n",
      "PARENT TEXT: O R . s c [\n",
      "    CHILD TYPE: UncategorizedText --> NUMBER OF CHILDREN: 2\n",
      "\n",
      "PARENT TYPE: Title, PARENT ID: 7e8a689d89b024a7dc47ebee59bb07fa\n",
      "PARENT TEXT: III. THE OPEN X-EMBODIMENT REPOSITORY\n",
      "    CHILD TYPE: NarrativeText --> NUMBER OF CHILDREN: 4\n",
      "\n",
      "PARENT TYPE: Title, PARENT ID: 7e8a689d89b024a7dc47ebee59bb07fa\n",
      "PARENT TEXT: III. THE OPEN X-EMBODIMENT REPOSITORY\n",
      "    CHILD TYPE: ListItem --> NUMBER OF CHILDREN: 2\n",
      "\n",
      "PARENT TYPE: Title, PARENT ID: 482139f50b98da4165176eea35acb871\n",
      "PARENT TEXT: B. Dataset Analysis\n",
      "    CHILD TYPE: NarrativeText --> NUMBER OF CHILDREN: 1\n",
      "\n",
      "PARENT TYPE: Title, PARENT ID: e110774179bc4d92f2382f2063c16a2f\n",
      "PARENT TEXT: IV. RT-X DESIGN\n",
      "    CHILD TYPE: NarrativeText --> NUMBER OF CHILDREN: 1\n",
      "\n",
      "PARENT TYPE: Title, PARENT ID: 6ffcbfd5e933ece63fe9d715a734707c\n",
      "PARENT TEXT: A. Data format consolidation\n",
      "    CHILD TYPE: NarrativeText --> NUMBER OF CHILDREN: 8\n",
      "\n",
      "PARENT TYPE: Title, PARENT ID: 6ffcbfd5e933ece63fe9d715a734707c\n",
      "PARENT TEXT: A. Data format consolidation\n",
      "    CHILD TYPE: ListItem --> NUMBER OF CHILDREN: 1\n",
      "\n",
      "PARENT TYPE: Title, PARENT ID: 0200b826f862b4f75389fd989b474835\n",
      "PARENT TEXT: REFERENCES\n",
      "    CHILD TYPE: ListItem --> NUMBER OF CHILDREN: 52\n",
      "\n",
      "PARENT TYPE: Title, PARENT ID: 0200b826f862b4f75389fd989b474835\n",
      "PARENT TEXT: REFERENCES\n",
      "    CHILD TYPE: NarrativeText --> NUMBER OF CHILDREN: 14\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# display parent and child relationship structure\n",
    "inspect.print_child_records()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two tables. I'm interested to know how well the model did at inferring the table structure and extracting the text.\n",
    "- Table 1: The table structure is perfect. There are 6 rows and 4 cols. Only one text entry is incorrect. '9i%' should be '92%'\n",
    "- Table 2: Again, the table structure is perfect. There are 8 rows and 9 cols. However, much of the Row and Model columns are incorrect. The Row column isn't immportant for retrieval, but the Model column is. Every row below the first should be 'RT-2-X' instead of the varied entries below. \n",
    "\n",
    "**Takeaway: Text extraction from tables is nearly flawless. My sense is that dashes throw off the OCR because in the cases where there were issues, dashes either preceeded or were embedded in the text.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<unstructured.documents.elements.Table at 0x711b806aa020>,\n",
       " <unstructured.documents.elements.Table at 0x711b806aa5c0>]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# there are two tables extracted from the PDF\n",
    "tables = [el for el in elements if el.category == \"Table\"]\n",
    "tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_23101/1287903928.py:2: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display, HTML\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table><thead><tr><th>Evaluation Setting</th><th>Bridge</th><th>Bridge</th><th>RT-1 paper 6 skills</th></tr></thead><tbody><tr><td>Evaluation Location Robot Embodiment Original Method</td><td>RIS (Stanford) WidowX LCBC [95]</td><td>RAIL Lab (UCB) WidowX LCBC [95]</td><td>Google Robotic Lab Google Robot</td></tr><tr><td>Original Method</td><td>13%</td><td>13%</td><td></td></tr><tr><td>RT-1</td><td>40%</td><td>30%</td><td>9i%</td></tr><tr><td>RT-1-X</td><td>27%</td><td>27%</td><td>73%</td></tr><tr><td>RT-2-X (55B)</td><td>50%</td><td>30%</td><td>91%</td></tr></tbody></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table><thead><tr><th>Row</th><th>Model</th><th>Size</th><th>History Length</th><th>Dataset</th><th>Co-Trained w/ Web</th><th>Initial Checkpoint</th><th>Emergent Skills Evaluation</th><th>RT-2 Generalization Evaluation</th></tr></thead><tbody><tr><td>(6]</td><td>RT-2</td><td>55B</td><td>none</td><td>Google Robot action</td><td>Yes</td><td>Web-pretrained</td><td>27.3%</td><td>62%</td></tr><tr><td>)</td><td>RT-2-X</td><td>55B</td><td>none</td><td>Robotics data</td><td>Yes</td><td>Web-pretrained</td><td>75.8%</td><td>61%</td></tr><tr><td>3)</td><td>RT-2-X</td><td>55B</td><td>none</td><td>Robotics data except Bridge</td><td>Yes</td><td>Web-pretrained</td><td>42.8%</td><td>54%</td></tr><tr><td>“)</td><td>RT-</td><td>5B</td><td>2</td><td>Robotics data</td><td>Yes</td><td>Web-pretrained</td><td>44.4%</td><td>52%</td></tr><tr><td>)</td><td>RT-2-</td><td>5B</td><td>none</td><td>Robotics data</td><td>Yes</td><td>Web-pretrained</td><td>14.5%</td><td>30%</td></tr><tr><td>©6)</td><td>RT-2-X</td><td>5B</td><td>2</td><td>Robotics data</td><td>No</td><td>From scratch</td><td>0%</td><td>1%</td></tr><tr><td>7</td><td>RT-2-.</td><td>5B</td><td>2</td><td>Robotics data</td><td>No</td><td>Web-pretrained</td><td>48.7%</td><td>47%</td></tr></tbody></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# let's see how well the DLD model did at inferring the table structure and translating the visual content to text.\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "table_html_1 = tables[0].metadata.text_as_html\n",
    "table_html_2 = tables[1].metadata.text_as_html\n",
    "\n",
    "display(HTML(str(table_html_1)))\n",
    "display(HTML(str(table_html_2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finish Preprocessing\n",
    "As a result of inspecting the data, I decided to remove references and header section content (if any) from the output that will be used for retrieval. You can see that ListItem type elements are reduced by the 52 list items that were in References and the UncategorizedText type elements are removed. \n",
    "\n",
    "I then converted the final element objects to records and added section titles to metadata with the add_parent_to_metadata function. Metadata is important because it can be used explicitly in the query with hybrid search or implicitly with chunking. \n",
    "\n",
    "Hybrid search combines semantic search with a structured query. For example, I could ask the LLM a question about the RTX paper and include a WHERE clause in the context that filters the retrieval to a particular section by using the 'section' key I added to the metadata. An enterprise relevant use case might be filtering to the most recent policy & procedure document for employee onboarding queries. This would require a last updated or created date in the metadata.\n",
    "\n",
    "Alternatively, metadata can be leveraged implicitly with chunking. This is the approach that's most relevant for my use case. A basic chunking strategy is naive - chunks can include text up to a certain character limit. A chunking strategy informed by metadata can improve response quality because chunks only include topically relevant text. Unstructured.io has a convenient chunk_by_title method that uses the parent_id to chunk section content under the relevant title (parent). The drawback is the vision model correctly identified only 5 of 15 section titles for the RTX paper, so perhaps this strategy isn't as impactful as it could be in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Header ID not found.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('NarrativeText', 50),\n",
       " ('ListItem', 49),\n",
       " ('Title', 6),\n",
       " ('FigureCaption', 5),\n",
       " ('Table', 2)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filter out irrelevant categories from pdf_elements\n",
    "filter_category_list = ['image','uncategorizedtext']\n",
    "pdf_elements = [el for el in elements if el.category.lower() not in (filter_category_list)]\n",
    "\n",
    "# get parent_id of child elements that belong to header and references Title sections \n",
    "header_id, references_id = inspect.get_references_and_header_id(records)\n",
    "\n",
    "# filter child elements from the references and header section\n",
    "pdf_elements = [el for el in pdf_elements if el.metadata.parent_id not in (references_id, header_id)]\n",
    "\n",
    "inspect.count_elements(elements=pdf_elements)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are still 6 Title sections, which means the 'References' title is still present (although it's children have been removed). It should be possible to remove these parent elements by accessing the element_id attribute, but it failed. This may be an issue with the unstructured.io class. Converting the elements to records and operating on the record is an easy work around. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1. convert elements to records, 2. add section title to metadata\n",
    "pdf_records_with_parent = doc.add_parent_to_metadata(pdf_elements, section_ids)\n",
    "\n",
    "# remove references and header parent elements \n",
    "# Note: this should be possible by accessing the Element class attributes (ei. element_id), but didn't work for some reason.\n",
    "#   Perhaps there's an issue with the unstructured.io class. Fortulately, operating on the record instead is easy work around. \n",
    "pdf_records_with_parent = [record for record in pdf_records_with_parent if record['element_id'] not in (references_id, header_id)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved successfully to data.json\n"
     ]
    }
   ],
   "source": [
    "# save records as JSON\n",
    "utils.save_json_line_by_line('data.json', pdf_records_with_parent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully from data.json\n"
     ]
    }
   ],
   "source": [
    "# load json to work from saved data\n",
    "data = utils.load_json_line_by_line('data.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'element_id': '6821f000be4d925646adb547846faa3a',\n",
      " 'metadata': {'filename': 'rtx_paper.pdf',\n",
      "              'filetype': 'application/pdf',\n",
      "              'languages': ['eng'],\n",
      "              'page_number': 3,\n",
      "              'parent_id': '7e8a689d89b024a7dc47ebee59bb07fa',\n",
      "              'section': 'III. THE OPEN X-EMBODIMENT REPOSITORY'},\n",
      " 'text': '• Open X-Embodiment Dataset: robot learning dataset with 1M+ robot '\n",
      "         'trajectories from 22 robot embodi- ments.',\n",
      " 'type': 'ListItem'}\n"
     ]
    }
   ],
   "source": [
    "# check a random record to make sure section name added to metadata\n",
    "for record in data:\n",
    "    if record['type'].lower()== 'listitem' and record['element_id'] == '6821f000be4d925646adb547846faa3a':\n",
    "        pprint(record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PARENT TYPE: Title, PARENT ID: 7e8a689d89b024a7dc47ebee59bb07fa\n",
      "PARENT TEXT: III. THE OPEN X-EMBODIMENT REPOSITORY\n",
      "    CHILD TYPE: NarrativeText --> NUMBER OF CHILDREN: 4\n",
      "\n",
      "PARENT TYPE: Title, PARENT ID: 7e8a689d89b024a7dc47ebee59bb07fa\n",
      "PARENT TEXT: III. THE OPEN X-EMBODIMENT REPOSITORY\n",
      "    CHILD TYPE: ListItem --> NUMBER OF CHILDREN: 2\n",
      "\n",
      "PARENT TYPE: Title, PARENT ID: 482139f50b98da4165176eea35acb871\n",
      "PARENT TEXT: B. Dataset Analysis\n",
      "    CHILD TYPE: NarrativeText --> NUMBER OF CHILDREN: 1\n",
      "\n",
      "PARENT TYPE: Title, PARENT ID: e110774179bc4d92f2382f2063c16a2f\n",
      "PARENT TEXT: IV. RT-X DESIGN\n",
      "    CHILD TYPE: NarrativeText --> NUMBER OF CHILDREN: 1\n",
      "\n",
      "PARENT TYPE: Title, PARENT ID: 6ffcbfd5e933ece63fe9d715a734707c\n",
      "PARENT TEXT: A. Data format consolidation\n",
      "    CHILD TYPE: NarrativeText --> NUMBER OF CHILDREN: 8\n",
      "\n",
      "PARENT TYPE: Title, PARENT ID: 6ffcbfd5e933ece63fe9d715a734707c\n",
      "PARENT TEXT: A. Data format consolidation\n",
      "    CHILD TYPE: ListItem --> NUMBER OF CHILDREN: 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Note: does not show parentless child elements\n",
    "inspect.print_child_records(records=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unstructured.chunking.title import chunk_by_title\n",
    "from unstructured.staging.base import dict_to_elements\n",
    "\n",
    "elements_with_parent = dict_to_elements(data)\n",
    "\n",
    "chunks = chunk_by_title(\n",
    "    elements_with_parent,\n",
    "    combine_text_under_n_chars=100,\n",
    "    max_characters=3000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.python/current/lib/python3.10/site-packages/IPython/core/display.py:618: UserWarning: JSON expects JSONable dict or list, not JSON strings\n",
      "  warnings.warn(\"JSON expects JSONable dict or list, not JSON strings\")\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "element_id": "d39b965e-53d0-4ff2-a3a4-949967a56f0f",
       "metadata": {
        "filename": "rtx_paper.pdf",
        "filetype": "application/pdf",
        "languages": [
         "eng"
        ],
        "orig_elements": "eJzNlm+L20YQxr/K4Fct2K4lWZKdNyEECoWQlnC0gTQcs9qRtL3Vrthd++yGfvfOruQ7X3oE8qbcS++/mXnmN4/86cuCNA1kwq2Si1ewyNuqaKlq6lrmu2yfIWWy2kskzPebvNgulrAYKKDEgHz+y6JVmgwOFC+7cLodcSS3HmUbT8bNcB7TJo6jVg0GZc1P87ZG0x2wI8/7nxZkusVnXh155dYcBkGO1zNe8dTEa/GVxT/8O9AppHgwwhvI41OXKO/ROY5xpJt4hg9/XWBdlE2FbUGF3OG+qjd5ThL3W7mnbFts2xdV4K/wAdbgoeHbV0XeqKDpueKynWhEjUW9LQvalbhDKcpSZhlthKzxZRX3s+rWkL2CPwhGZ0frCdCAHcksQaPraOUb1AQxWU8BWuvAWWEDaEJnlOmgOXCzSULr7AB5Bsr4oMIhhvOAjbPeQ+gJOm0FreGmf3zN0ejIs3QeJOPiOLqgHo/KOr+c49AgrFRDOoRGApmjctakheW8gkKTf8yoI0MOtfqbs0qPqAZGy9Ip8uvrHnL1B0dvcUzaPNNL3DTVdlPV+V5s97t8X9RVjiSybb3dlW2NL6qXb4QPDpvw5yHfZFt4F9u3hF51/arBERsVzjBYSZr74VAZVseaB+Hnnnhg/Ql8b+8N92dAdxfVBX9oGvKeZeY71LZRTBP0GQI2dzqqLvkGJ0A4wFVxfg2/cBA7cEA/MfL+3W8QLLy1w3gI5OB35fngkiFRMXpspIwHEBq+z32T6SWwLTNKl9SnSpZwr0J/6fj1vuC8hDWcsCd3jAnyywg+oAvx12iVmXge0Jy/SvktDwFX3P8nh57PkWHGL2D511yIObIWvI1an5cPyD3wyIT0Vs6qxxxoxDgzUw0pB+ImPEliOS+lx2bOOcp32WwlqShLEmXeirKRRZntml1R5ds2lxuqsxdF71Tu1XRPXbinZCiEcpYv0d1cRtxPtEv4uJrsIs05U9ljgIbvCzY0yQPORDyh1oKh+0ldhojRv3twk0d/eR3ZTVgmJZYxG7bJo5JX88I5MVRGopPJceJG7OmAs2NdZs4yaXdcTkzSexWnitfoNGrraA6TNpSOo6pMsk3mL0oU6b+AxcCq8aBnTlBb0/mYEj9FLlkljwIb60EHPylxSZoFoRMOI4fm91iQ2Awe9yfqRZeMHwTkcR9ilvho/8njczYNvuo40KwgZ6k1vxUHt3f20E2TozUK66bJERTuiUcnz76L4VLWVNVVXQjaE5VZmdWyysuiEZtyS2X5vzOcf4Ph60/fEiQNNjoiJr8p8xr8ndJMwg9ZtcmraqLux6R19NupVficY18bdgIllss49oo9iqHkb7SGDzerj2wbp54JCokllZrLd43ndiUY1RBZoOmTHIMk2lT09RYGJkZFOOauijObGM8ldrGCCS8yDU0cWH6C//0xh5F2P33a+fW/WB1OSnB4pt0/OOVqSoTPklud1h0b90GslV1/A4jP/wKM5Mof",
        "page_number": 1
       },
       "text": "r p A 2\n\nO R . s c [\n\nFig. 1: We propose an open, large-scale dataset for robot learning curated from 21 institutions across the globe. The dataset represents diverse behaviors, robot embodiments and environments, and enables learning generalized robotic policies.\n\nAbstract— Large, high-capacity models trained on diverse datasets have shown remarkable successes on efficiently tackling downstream applications. In domains from NLP to Computer Vision, this has led to a consolidation of pretrained models, with general pretrained backbones serving as a starting point for many applications. Can such a consolidation happen in robotics? Conventionally, robotic learning methods train a separate model for every application, every robot, and even\n\nevery environment. Can we instead train “generalist” X-robot policy that can be adapted efficiently to new robots, tasks, and environments? In this paper, we provide datasets in standardized data formats and models to make it possible to explore this possibility in the context of robotic manipulation, alongside experimental results that provide an example of effective X-robot policies. We assemble a dataset from 22 different robots collected through a collaboration between 21\n\ninstitutions, demonstrating 527 skills (160266 tasks). We show that a high-capacity model trained on this data, which we call RT-X, exhibits positive transfer and improves the capabilities of multiple robots by leveraging experience from other platforms. The project website is robotics-transformer-x.github.io.",
       "type": "CompositeElement"
      },
      "text/plain": [
       "<IPython.core.display.JSON object>"
      ]
     },
     "execution_count": 21,
     "metadata": {
      "application/json": {
       "expanded": false,
       "root": "root"
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import JSON\n",
    "import json\n",
    "\n",
    "# take a look at a chunk\n",
    "JSON(json.dumps(chunks[0].to_dict(), indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexing with Llama Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index douments \n",
    "from llama_index.core import VectorStoreIndex, ServiceContext, Document\n",
    "from llama_index.llms import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the chunks into one document to search over\n",
    "document = Document(text='\\n\\n'.join([chunk.text for chunk in chunks]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_23101/2810759234.py:7: DeprecationWarning: Call to deprecated class method from_defaults. (ServiceContext is deprecated, please use `llama_index.settings.Settings` instead.) -- Deprecated since version 0.10.0.\n",
      "  service_context = ServiceContext.from_defaults(llm=llm, embed_model='local:BAAI/bge-small-en-v1.5')\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0f2f941bea74feda6ce31703fcccafa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dde5f00c2d6641ebac7bc8f9e350a9db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba05fb6f9c1a4bc39049319a8fcac924",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/94.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "747988b1cf6b4477aecaa93293686964",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.python/current/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7bbb3aa09234ee29427ea682a460f6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/743 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca53111629494d6c9539f6044320ae93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/133M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afe407304fb4436db69b010da8fd61ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/366 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bc95830d94d46c295f303bf424d0d61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d79578c885b483eb0686eb5d07dc7b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/711k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75c426afcdb44969802f2af266eb563b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b086abe74584990838628453efa1024",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# get Open AI API key\n",
    "OPEN_API_KEY = utils.get_api_key(\"OPENAI\")\n",
    "openai.api_key = OPEN_API_KEY\n",
    "\n",
    "# define a service context that contains both the llm and the embedding model\n",
    "llm = openai.OpenAI(model='gpt-3.5-turbo', temperature=0.1)\n",
    "service_context = ServiceContext.from_defaults(llm=llm, embed_model='local:BAAI/bge-small-en-v1.5')\n",
    "index = VectorStoreIndex.from_documents([document], service_context=service_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('X-embodiment training involves training a policy on data from multiple robot '\n",
      " 'embodiments without employing mechanisms to reduce the embodiment gap. The '\n",
      " 'goal is to leverage diverse data from various robot embodiments to achieve '\n",
      " 'positive transfer and improve the range of tasks that can be performed by a '\n",
      " 'robot.')\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\n",
    "    \"what is x-embodiment training?\"\n",
    ")\n",
    "\n",
    "pprint(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
